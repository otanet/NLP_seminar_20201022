{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AllenNLPによる自然言語処理 (2): 畳み込みニューラルネットワークによる感情分析",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/otanet/NLP_seminar_20201022/blob/main/AllenNLP%E3%81%AB%E3%82%88%E3%82%8B%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86_(2)_%E7%95%B3%E3%81%BF%E8%BE%BC%E3%81%BF%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E3%81%AB%E3%82%88%E3%82%8B%E6%84%9F%E6%83%85%E5%88%86%E6%9E%90.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22nxw_bUcdRm"
      },
      "source": [
        "# AllenNLPによる自然言語処理 (2): 畳み込みニューラルネットワークによる感情分析"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_Z_edhCmDSr"
      },
      "source": [
        "**他のノートブックへのリンク：**\n",
        "\n",
        "* [AllenNLPによる自然言語処理 (1): bag-of-embeddingsモデルによる文書分類](https://colab.research.google.com/drive/1yquPsCgv7EpNPheqt_Th-j-kK3CN2VWQ?usp=sharing)\n",
        "* [AllenNLPによる自然言語処理 (3): BERTによる固有表現認識](https://colab.research.google.com/drive/13ga1yYYZkosGZy9ZinAB76blb-8k6yby?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jHSXQudchHT"
      },
      "source": [
        "このノートブックでは、ディープラーニングを用いた自然言語処理のためのツールである[AllenNLP](https://allennlp.org/)を用いて、**畳み込みニューラルネットワーク（CNN）**を用いた**感情分析**のシステムを作成します。\n",
        "\n",
        "感情分析は、テキストに紐付いた感情を分析する基本的な自然言語処理のタスクの1つです。\n",
        "アンケート結果の集計やソーシャルメディアの解析などの実用的な用途で、幅広く使用されています。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWHrIRahkI-g"
      },
      "source": [
        "## 環境のセットアップ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUjcZ6D4uzLU"
      },
      "source": [
        "本ノートブックの実行に必要なパッケージをインストールします。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEDk6RxI_jLG",
        "outputId": "ce03c03a-e74b-456d-b8b8-1c7aa5fcfdef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# MeCabをインストール\n",
        "!apt-get -qq install mecab swig libmecab-dev mecab-ipadic-utf8\n",
        "# MeCabのPythonバインディングとAllenNLPをインストール\n",
        "# （boto3はAllenNLPの依存ライブラリだが、最新バージョンだとエラーになるためバージョンを指定）\n",
        "!pip install \"mecab-python3==0.996.5\" \"allennlp==1.1.0\" \"boto3==1.15.0\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selecting previously unselected package libmecab2:amd64.\n",
            "(Reading database ... 144611 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libmecab2_0.996-5_amd64.deb ...\n",
            "Unpacking libmecab2:amd64 (0.996-5) ...\n",
            "Selecting previously unselected package libmecab-dev.\n",
            "Preparing to unpack .../1-libmecab-dev_0.996-5_amd64.deb ...\n",
            "Unpacking libmecab-dev (0.996-5) ...\n",
            "Selecting previously unselected package mecab-utils.\n",
            "Preparing to unpack .../2-mecab-utils_0.996-5_amd64.deb ...\n",
            "Unpacking mecab-utils (0.996-5) ...\n",
            "Selecting previously unselected package mecab-jumandic-utf8.\n",
            "Preparing to unpack .../3-mecab-jumandic-utf8_7.0-20130310-4_all.deb ...\n",
            "Unpacking mecab-jumandic-utf8 (7.0-20130310-4) ...\n",
            "Selecting previously unselected package mecab-jumandic.\n",
            "Preparing to unpack .../4-mecab-jumandic_7.0-20130310-4_all.deb ...\n",
            "Unpacking mecab-jumandic (7.0-20130310-4) ...\n",
            "Selecting previously unselected package mecab-ipadic.\n",
            "Preparing to unpack .../5-mecab-ipadic_2.7.0-20070801+main-1_all.deb ...\n",
            "Unpacking mecab-ipadic (2.7.0-20070801+main-1) ...\n",
            "Selecting previously unselected package mecab.\n",
            "Preparing to unpack .../6-mecab_0.996-5_amd64.deb ...\n",
            "Unpacking mecab (0.996-5) ...\n",
            "Selecting previously unselected package mecab-ipadic-utf8.\n",
            "Preparing to unpack .../7-mecab-ipadic-utf8_2.7.0-20070801+main-1_all.deb ...\n",
            "Unpacking mecab-ipadic-utf8 (2.7.0-20070801+main-1) ...\n",
            "Selecting previously unselected package swig3.0.\n",
            "Preparing to unpack .../8-swig3.0_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig3.0 (3.0.12-1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../9-swig_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig (3.0.12-1) ...\n",
            "Setting up swig3.0 (3.0.12-1) ...\n",
            "Setting up libmecab2:amd64 (0.996-5) ...\n",
            "Setting up mecab-utils (0.996-5) ...\n",
            "Setting up mecab-ipadic (2.7.0-20070801+main-1) ...\n",
            "Compiling IPA dictionary for Mecab.  This takes long time...\n",
            "reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n",
            "reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n",
            "reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n",
            "reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n",
            "reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n",
            "reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27327\n",
            "reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n",
            "reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n",
            "reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n",
            "reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n",
            "reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n",
            "reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n",
            "reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n",
            "reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n",
            "reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "update-alternatives: using /var/lib/mecab/dic/ipadic to provide /var/lib/mecab/dic/debian (mecab-dictionary) in auto mode\n",
            "Setting up swig (3.0.12-1) ...\n",
            "Setting up libmecab-dev (0.996-5) ...\n",
            "Setting up mecab-jumandic-utf8 (7.0-20130310-4) ...\n",
            "Compiling Juman dictionary for Mecab.\n",
            "reading /usr/share/mecab/dic/juman/unk.def ... 37\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/juman/Postp.csv ... 108\n",
            "reading /usr/share/mecab/dic/juman/Assert.csv ... 34\n",
            "reading /usr/share/mecab/dic/juman/Noun.hukusi.csv ... 81\n",
            "reading /usr/share/mecab/dic/juman/Noun.keishiki.csv ... 8\n",
            "reading /usr/share/mecab/dic/juman/Rengo.csv ... 1118\n",
            "reading /usr/share/mecab/dic/juman/Wikipedia.csv ... 167709\n",
            "reading /usr/share/mecab/dic/juman/Special.csv ... 158\n",
            "reading /usr/share/mecab/dic/juman/Demonstrative.csv ... 97\n",
            "reading /usr/share/mecab/dic/juman/AuxV.csv ... 593\n",
            "reading /usr/share/mecab/dic/juman/Prefix.csv ... 90\n",
            "reading /usr/share/mecab/dic/juman/Suffix.csv ... 2128\n",
            "reading /usr/share/mecab/dic/juman/ContentW.csv ... 551145\n",
            "reading /usr/share/mecab/dic/juman/Auto.csv ... 18931\n",
            "reading /usr/share/mecab/dic/juman/Emoticon.csv ... 972\n",
            "reading /usr/share/mecab/dic/juman/Noun.koyuu.csv ... 7964\n",
            "reading /usr/share/mecab/dic/juman/Noun.suusi.csv ... 49\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/juman/matrix.def ... 1876x1876\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "Setting up mecab-ipadic-utf8 (2.7.0-20070801+main-1) ...\n",
            "Compiling IPA dictionary for Mecab.  This takes long time...\n",
            "reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n",
            "reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n",
            "reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n",
            "reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n",
            "reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n",
            "reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27327\n",
            "reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n",
            "reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n",
            "reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n",
            "reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n",
            "reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n",
            "reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n",
            "reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n",
            "reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n",
            "reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "update-alternatives: using /var/lib/mecab/dic/ipadic-utf8 to provide /var/lib/mecab/dic/debian (mecab-dictionary) in auto mode\n",
            "Setting up mecab (0.996-5) ...\n",
            "Compiling IPA dictionary for Mecab.  This takes long time...\n",
            "reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n",
            "reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n",
            "reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n",
            "reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n",
            "reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n",
            "reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27327\n",
            "reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n",
            "reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n",
            "reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n",
            "reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n",
            "reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n",
            "reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n",
            "reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n",
            "reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n",
            "reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "Setting up mecab-jumandic (7.0-20130310-4) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Collecting mecab-python3==0.996.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/49/b55a839a77189042960bf96490640c44816073f917d489acbc5d79fa5cc3/mecab_python3-0.996.5-cp36-cp36m-manylinux2010_x86_64.whl (17.1MB)\n",
            "\u001b[K     |████████████████████████████████| 17.1MB 175kB/s \n",
            "\u001b[?25hCollecting allennlp==1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/95/d1d606fff85b537ba6dd133ed998ab62bf0c950feb6df2d101c0ec804ca6/allennlp-1.1.0-py3-none-any.whl (485kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 53.8MB/s \n",
            "\u001b[?25hCollecting boto3==1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/a1/d9e77245939ec608f0787545824356705f2341c395c5a37e9778bdb1cd98/boto3-1.15.0-py2.py3-none-any.whl (129kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 55.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch<1.7.0,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from allennlp==1.1.0) (1.6.0+cu101)\n",
            "Collecting tensorboardX>=1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 55.4MB/s \n",
            "\u001b[?25hCollecting jsonnet>=0.10.0; sys_platform != \"win32\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/70/ed1ba808a87d896b9f4d25400dda54e089ca7a97e87cee620b3744997c89/jsonnet-0.16.0.tar.gz (256kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 43.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from allennlp==1.1.0) (0.7)\n",
            "Collecting transformers<3.1,>=3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 50.3MB/s \n",
            "\u001b[?25hCollecting overrides==3.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/ff/b1/10f69c00947518e6676bbd43e739733048de64b8dd998e9c2d5a71f44c5d/overrides-3.1.0.tar.gz\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp==1.1.0) (3.2.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from allennlp==1.1.0) (1.4.1)\n",
            "Requirement already satisfied: filelock<3.1,>=3.0 in /usr/local/lib/python3.6/dist-packages (from allennlp==1.1.0) (3.0.12)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp==1.1.0) (2.10.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from allennlp==1.1.0) (3.6.4)\n",
            "Requirement already satisfied: spacy<2.4,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from allennlp==1.1.0) (2.2.4)\n",
            "Collecting jsonpickle\n",
            "  Downloading https://files.pythonhosted.org/packages/af/ca/4fee219cc4113a5635e348ad951cf8a2e47fed2e3342312493f5b73d0007/jsonpickle-1.4.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.6/dist-packages (from allennlp==1.1.0) (4.41.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from allennlp==1.1.0) (0.22.2.post1)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.6/dist-packages (from allennlp==1.1.0) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from allennlp==1.1.0) (1.18.5)\n",
            "Collecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.8MB/s \n",
            "\u001b[?25hCollecting botocore<1.19.0,>=1.18.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2d/72/984ac8f33b5c8df5ff63f323a8724f65b4d0f8956968b942b77d35d3a1ef/botocore-1.18.18-py2.py3-none-any.whl (6.7MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7MB 49.0MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch<1.7.0,>=1.6.0->allennlp==1.1.0) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp==1.1.0) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp==1.1.0) (3.12.4)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 49.1MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 49.5MB/s \n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 47.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers<3.1,>=3.0->allennlp==1.1.0) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers<3.1,>=3.0->allennlp==1.1.0) (20.4)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp==1.1.0) (1.9.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp==1.1.0) (8.5.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp==1.1.0) (0.7.1)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp==1.1.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp==1.1.0) (20.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp==1.1.0) (50.3.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp==1.1.0) (7.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp==1.1.0) (1.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp==1.1.0) (0.8.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp==1.1.0) (0.4.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp==1.1.0) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp==1.1.0) (3.0.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp==1.1.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp==1.1.0) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp==1.1.0) (2.0.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/dist-packages (from jsonpickle->allennlp==1.1.0) (2.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->allennlp==1.1.0) (0.16.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp==1.1.0) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp==1.1.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp==1.1.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp==1.1.0) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.19.0,>=1.18.0->boto3==1.15.0) (2.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers<3.1,>=3.0->allennlp==1.1.0) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers<3.1,>=3.0->allennlp==1.1.0) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->jsonpickle->allennlp==1.1.0) (3.2.0)\n",
            "Building wheels for collected packages: jsonnet, overrides, sacremoses\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.16.0-cp36-cp36m-linux_x86_64.whl size=3321607 sha256=f4e4b20f05c3b58c0eb9d3e74c23ae4247dfe8e311602f8de1e7d4e5f062e449\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/a9/43/bc5e0463deeec89dfca928a2a64595f1bdb520c891f6fbd09c\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-cp36-none-any.whl size=10174 sha256=fd4191265fc907fad37eb3eaf93a366bb16efe1ee5ae6693c2bfe99b5a93760e\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/24/13/6ef8600e6f147c95e595f1289a86a3cc82ed65df57582c65a9\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=01e42e83e20087889b88c0157a33bd924d321531e8c8258a624e28d17d113cac\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built jsonnet overrides sacremoses\n",
            "Installing collected packages: mecab-python3, tensorboardX, jmespath, botocore, s3transfer, boto3, jsonnet, sacremoses, tokenizers, sentencepiece, transformers, overrides, jsonpickle, allennlp\n",
            "Successfully installed allennlp-1.1.0 boto3-1.15.0 botocore-1.18.18 jmespath-0.10.0 jsonnet-0.16.0 jsonpickle-1.4.1 mecab-python3-0.996.5 overrides-3.1.0 s3transfer-0.3.3 sacremoses-0.0.43 sentencepiece-0.1.91 tensorboardX-2.1 tokenizers-0.8.1rc1 transformers-3.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-rX41OYgSsN"
      },
      "source": [
        "## データセットのセットアップ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjsW0sySgVM_"
      },
      "source": [
        "本ノートブックでは、[Amazon Customer Reviewsデータセット](https://s3.amazonaws.com/amazon-reviews-pds/readme.html)というAmazonに蓄積されたユーザによる製品のレビューのデータセットを使います。\n",
        "\n",
        "各レビューにはユーザが指定した1〜5の5段階の評価が数値で付与されています。\n",
        "本章では1と2を否定的、4と5を肯定的であると考えて、レビューのテキストの内容からユーザが該当する製品に対して肯定的か否定的かを判別する感情分析のモデルの実装を行います。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUwwnajtkWeE"
      },
      "source": [
        "### データセットのダウンロード"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOL5DACg83lt"
      },
      "source": [
        "データセットをダウンロードして展開します。\n",
        "\n",
        "下記のコマンドによって、`data/amazon_reviews`に`amazon_reviews_multilingual_JP_v1_00.tsv`が出力されます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7So8lLxGFQPl"
      },
      "source": [
        "# データセットの出力ディレクトリを作成\n",
        "!mkdir -p data/amazon_reviews\n",
        "# データセットをダウンロード\n",
        "!wget -q -O data/amazon_reviews/amazon_reviews_multilingual_JP_v1_00.tsv.gz \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_multilingual_JP_v1_00.tsv.gz\"\n",
        "# データセットを解凍し、data/amazon_reviewsに展開\n",
        "!gunzip data/amazon_reviews/amazon_reviews_multilingual_JP_v1_00.tsv.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nj6sbj0NkaPr"
      },
      "source": [
        "### データセットを行区切りJSON形式に変換"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rghn5hJuAvQK"
      },
      "source": [
        "データセットをAllenNLPの`TextClassificationJsonReader`で読み込める[行区切りJSON（JSON lines）](https://jsonlines.org/)形式に変換します。\n",
        "このスクリプトはデータセットを読み込んで8:1:1の割合でそれぞれ訓練用、検証用、テスト用に分割し、保存します。\n",
        "\n",
        "また、このデータセットはウェブサイトから取得されたものであるため、レビューのテキストにはHTMLタグが含まれています。\n",
        "HTMLタグは今回のタスクにおいては不要なので、HTMLを解析するライブラリである`BeautifulSoup`を用いて除去します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlajO8LGhhzf"
      },
      "source": [
        "import csv\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "import warnings\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# csvライブラリのフィールドの最大サイズを変更\n",
        "csv.field_size_limit(1000000)\n",
        "# BeautifulSoupの出力する警告を抑制\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"bs4\")\n",
        "\n",
        "# データセットをファイルから読み込む\n",
        "data = []\n",
        "with open(\"data/amazon_reviews/amazon_reviews_multilingual_JP_v1_00.tsv\") as f:\n",
        "    reader = csv.reader(f, delimiter=\"\\t\")\n",
        "    # 1行目はヘッダなので無視する\n",
        "    next(reader)\n",
        "    for r in reader:\n",
        "        # レビューのテキストを取得\n",
        "        review_body = r[13]\n",
        "        # レビューのテキストからHTMLタグを除去\n",
        "        review_body = BeautifulSoup(review_body, \"html.parser\").get_text()\n",
        "        # 評価の値を取得\n",
        "        ratings = int(r[7])\n",
        "        # 評価が2以下の場合に否定的、4以上の場合に肯定的と扱う\n",
        "        if ratings <= 2:\n",
        "            data.append(dict(text=review_body, label=\"negative\"))\n",
        "        elif ratings >= 4:\n",
        "            data.append(dict(text=review_body, label=\"positive\"))\n",
        "\n",
        "# データセットから50,000件をランダムに抽出する\n",
        "random.seed(1)\n",
        "random.shuffle(data)\n",
        "data = data[:50000]\n",
        "\n",
        "# データセットの80%を訓練データ、10%を検証データ、10%をテストデータとして用いる\n",
        "split_data = {}\n",
        "eval_size = int(len(data) * 0.1)\n",
        "split_data[\"test\"] = data[:eval_size]\n",
        "split_data[\"validation\"] = data[eval_size:eval_size * 2]\n",
        "split_data[\"train\"] = data[eval_size * 2:]\n",
        "\n",
        "# JSON Lines形式でデータセットを書き込む\n",
        "for fold in (\"train\", \"validation\", \"test\"):\n",
        "    out_file = os.path.join(\"data/amazon_reviews\", \"amazon_reviews_{}.jsonl\".format(fold))\n",
        "    with open(out_file, mode=\"w\") as f:\n",
        "        for item in split_data[fold]:\n",
        "            json.dump(item, f, ensure_ascii=False)\n",
        "            f.write(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1wRusYjiEVs"
      },
      "source": [
        "実行が終わると下記の3個のファイルが`data/amazon_reviews`に生成されます。\n",
        "\n",
        "* **訓練用のデータ**: `amazon_reviews_train.jsonl`\n",
        "* **検証用のデータ**: `amazon_reviews_validation.jsonl`\n",
        "* **テスト用のデータ**: `amazon_reviews_test.jsonl`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDmvwoaCiBBj",
        "outputId": "5da02fcd-1541-47b3-eeed-23cb25038c69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!ls data/amazon_reviews/*.jsonl"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data/amazon_reviews/amazon_reviews_test.jsonl\n",
            "data/amazon_reviews/amazon_reviews_train.jsonl\n",
            "data/amazon_reviews/amazon_reviews_validation.jsonl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Px3XgvljMIN"
      },
      "source": [
        "## 設定ファイルの作成"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_UnFTnWjmW-"
      },
      "source": [
        "下記にCNNを用いた感情分析のモデルの設定ファイルを示します。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4PJwlDzUxdA"
      },
      "source": [
        "model_config = \"\"\"{\n",
        "    \"random_seed\": 1,\n",
        "    \"pytorch_seed\": 1,\n",
        "    \"train_data_path\": \"data/amazon_reviews/amazon_reviews_train.jsonl\",\n",
        "    \"validation_data_path\": \"data/amazon_reviews/amazon_reviews_validation.jsonl\",\n",
        "    \"dataset_reader\": {\n",
        "        \"type\": \"text_classification_json\",\n",
        "        \"tokenizer\": {\n",
        "            \"type\": \"mecab\"\n",
        "        },\n",
        "        \"token_indexers\": {\n",
        "            \"tokens\": {\n",
        "                \"type\": \"single_id\"\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    \"vocabulary\": {},\n",
        "    \"datasets_for_vocab_creation\": [\"train\", \"validation\"],\n",
        "    \"data_loader\": {\n",
        "        \"batch_size\": 32,\n",
        "        \"shuffle\": true\n",
        "    },\n",
        "    \"validation_data_loader\": {\n",
        "        \"batch_size\": 32,\n",
        "        \"shuffle\": false\n",
        "    },\n",
        "    \"model\": {\n",
        "        \"type\": \"basic_classifier\",\n",
        "        \"text_field_embedder\": {\n",
        "            \"token_embedders\": {\n",
        "                \"tokens\": {\n",
        "                    \"type\": \"embedding\",\n",
        "                    \"embedding_dim\": 100,\n",
        "                    \"padding_index\": 0\n",
        "                }\n",
        "            }\n",
        "        },\n",
        "        \"seq2vec_encoder\": {\n",
        "           \"type\": \"cnn\",\n",
        "           \"embedding_dim\": 100,\n",
        "           \"ngram_filter_sizes\": [2],\n",
        "           \"num_filters\": 64\n",
        "        }\n",
        "    },\n",
        "    \"trainer\": {\n",
        "        \"optimizer\": {\n",
        "            \"type\": \"adam\"\n",
        "        },\n",
        "        \"num_epochs\": 10,\n",
        "        \"patience\": 3,\n",
        "        \"cuda_device\": 0\n",
        "    }\n",
        "}\"\"\"\n",
        "with open(\"amazon_reviews.jsonnet\", \"w\") as f:\n",
        "  f.write(model_config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYKFrdB_csrW"
      },
      "source": [
        "本ノートブックで扱う感情分析は、レビューのテキストを肯定的か否定的かに分類する文書分類の問題であるため、設定ファイルは[文書分類の設定ファイル](https://colab.research.google.com/drive/1yquPsCgv7EpNPheqt_Th-j-kK3CN2VWQ?usp=sharing)とモデルに関する設定以外は同一の内容になっています。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_lxFYuVkL3S"
      },
      "source": [
        "モデルに関する設定を見てみましょう。\n",
        "分類のモデルとして`BasicClassifier`（`basic_classifier`）を用います。\n",
        "\n",
        "```json\n",
        "    \"model\": {\n",
        "        \"type\": \"basic_classifier\",\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrgzgQFbkcIC"
      },
      "source": [
        "100次元の単語エンベディングを用いてテキストフィールドエンベダを作成します。\n",
        "\n",
        "\n",
        "```json\n",
        "        \"text_field_embedder\": {\n",
        "            \"token_embedders\": {\n",
        "                \"tokens\": {\n",
        "                    \"type\": \"embedding\",\n",
        "                    \"embedding_dim\": 100\n",
        "                }\n",
        "            }\n",
        "        },\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlofkYGmkrwx"
      },
      "source": [
        "seq2vecエンコーダとして、AllenNLPに実装されているCNNの実装である`CnnEncoder`（`cnn`）を使います。\n",
        "また`CnnEncoder`には窓幅（`ngram_filter_sizes`）を2（連続した2単語を考慮）、フィルタの数（`num_filters`）を64に設定します。\n",
        "\n",
        "```json\n",
        "        \"seq2vec_encoder\": {\n",
        "           \"type\": \"cnn\",\n",
        "           \"embedding_dim\": 100,\n",
        "           \"ngram_filter_sizes\": [2],\n",
        "           \"num_filters\": 64\n",
        "        }\n",
        "    }\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sw2tZUesloQ-"
      },
      "source": [
        "## モデルの訓練"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPMG0UThluN8"
      },
      "source": [
        "`allennlp train`コマンドを使ってモデルを訓練します。出力ディレクトリを`exp_amazon_reviews`ディレクトリに指定します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0CHam0uoevK",
        "outputId": "97aa5662-9d34-4955-d12f-db2fde1db921",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# MecabTokenizerのPythonファイルをダウンロード\n",
        "!wget -q https://dl.dropboxusercontent.com/s/2qeulihmlv8btbg/mecab_tokenizer.py\n",
        "# 出力ディレクトリが既にあった場合は削除\n",
        "!rm -rf exp_amazon_reviews\n",
        "# 訓練を実行\n",
        "!allennlp train --serialization-dir exp_amazon_reviews --include-package mecab_tokenizer amazon_reviews.jsonnet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-10-20 05:23:43.150781: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-10-20 05:23:44,392 - INFO - transformers.file_utils - TensorFlow version 2.3.0 available.\n",
            "2020-10-20 05:23:45,711 - INFO - allennlp.common.params - random_seed = 1\n",
            "2020-10-20 05:23:45,711 - INFO - allennlp.common.params - numpy_seed = 1337\n",
            "2020-10-20 05:23:45,711 - INFO - allennlp.common.params - pytorch_seed = 1\n",
            "2020-10-20 05:23:45,757 - INFO - allennlp.common.checks - Pytorch version: 1.6.0+cu101\n",
            "2020-10-20 05:23:45,759 - INFO - allennlp.common.params - type = default\n",
            "2020-10-20 05:23:45,760 - INFO - allennlp.common.params - dataset_reader.type = text_classification_json\n",
            "2020-10-20 05:23:45,760 - INFO - allennlp.common.params - dataset_reader.lazy = False\n",
            "2020-10-20 05:23:45,760 - INFO - allennlp.common.params - dataset_reader.cache_directory = None\n",
            "2020-10-20 05:23:45,760 - INFO - allennlp.common.params - dataset_reader.max_instances = None\n",
            "2020-10-20 05:23:45,760 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False\n",
            "2020-10-20 05:23:45,760 - INFO - allennlp.common.params - dataset_reader.manual_multi_process_sharding = False\n",
            "2020-10-20 05:23:45,760 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = single_id\n",
            "2020-10-20 05:23:45,761 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.namespace = tokens\n",
            "2020-10-20 05:23:45,761 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.lowercase_tokens = False\n",
            "2020-10-20 05:23:45,761 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.start_tokens = None\n",
            "2020-10-20 05:23:45,761 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.end_tokens = None\n",
            "2020-10-20 05:23:45,761 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.feature_name = text\n",
            "2020-10-20 05:23:45,761 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.default_value = THIS IS A REALLY UNLIKELY VALUE THAT HAS TO BE A STRING\n",
            "2020-10-20 05:23:45,761 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
            "2020-10-20 05:23:45,761 - INFO - allennlp.common.params - dataset_reader.tokenizer.type = mecab\n",
            "2020-10-20 05:23:45,763 - INFO - allennlp.common.params - dataset_reader.segment_sentences = False\n",
            "2020-10-20 05:23:45,763 - INFO - allennlp.common.params - dataset_reader.max_sequence_length = None\n",
            "2020-10-20 05:23:45,763 - INFO - allennlp.common.params - dataset_reader.skip_label_indexing = False\n",
            "2020-10-20 05:23:45,763 - INFO - allennlp.common.params - train_data_path = data/amazon_reviews/amazon_reviews_train.jsonl\n",
            "2020-10-20 05:23:45,763 - INFO - allennlp.common.params - datasets_for_vocab_creation = ['train', 'validation']\n",
            "2020-10-20 05:23:45,764 - INFO - allennlp.common.params - validation_dataset_reader = None\n",
            "2020-10-20 05:23:45,764 - INFO - allennlp.common.params - validation_data_path = data/amazon_reviews/amazon_reviews_validation.jsonl\n",
            "2020-10-20 05:23:45,764 - INFO - allennlp.common.params - test_data_path = None\n",
            "2020-10-20 05:23:45,764 - INFO - allennlp.common.params - evaluate_on_test = False\n",
            "2020-10-20 05:23:45,764 - INFO - allennlp.common.params - batch_weight_key = \n",
            "2020-10-20 05:23:45,764 - INFO - allennlp.training.util - Reading training data from data/amazon_reviews/amazon_reviews_train.jsonl\n",
            "reading instances: 40000it [00:33, 1180.03it/s]\n",
            "2020-10-20 05:24:19,662 - INFO - allennlp.training.util - Reading validation data from data/amazon_reviews/amazon_reviews_validation.jsonl\n",
            "reading instances: 5000it [00:03, 1540.73it/s]\n",
            "2020-10-20 05:24:22,908 - INFO - allennlp.commands.train - From dataset instances, train, validation will be considered for vocabulary creation.\n",
            "2020-10-20 05:24:22,908 - INFO - allennlp.common.params - vocabulary.type = from_instances\n",
            "2020-10-20 05:24:22,908 - INFO - allennlp.common.params - vocabulary.min_count = None\n",
            "2020-10-20 05:24:22,909 - INFO - allennlp.common.params - vocabulary.max_vocab_size = None\n",
            "2020-10-20 05:24:22,909 - INFO - allennlp.common.params - vocabulary.non_padded_namespaces = ('*tags', '*labels')\n",
            "2020-10-20 05:24:22,909 - INFO - allennlp.common.params - vocabulary.pretrained_files = None\n",
            "2020-10-20 05:24:22,909 - INFO - allennlp.common.params - vocabulary.only_include_pretrained_words = False\n",
            "2020-10-20 05:24:22,909 - INFO - allennlp.common.params - vocabulary.tokens_to_add = None\n",
            "2020-10-20 05:24:22,909 - INFO - allennlp.common.params - vocabulary.min_pretrained_embeddings = None\n",
            "2020-10-20 05:24:22,909 - INFO - allennlp.common.params - vocabulary.padding_token = @@PADDING@@\n",
            "2020-10-20 05:24:22,909 - INFO - allennlp.common.params - vocabulary.oov_token = @@UNKNOWN@@\n",
            "2020-10-20 05:24:22,909 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.\n",
            "building vocab: 45000it [00:04, 9305.78it/s]\n",
            "2020-10-20 05:24:27,967 - INFO - allennlp.common.params - model.type = basic_classifier\n",
            "2020-10-20 05:24:27,968 - INFO - allennlp.common.params - model.regularizer = None\n",
            "2020-10-20 05:24:27,968 - INFO - allennlp.common.params - model.text_field_embedder.type = basic\n",
            "2020-10-20 05:24:27,968 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.type = embedding\n",
            "2020-10-20 05:24:27,968 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.embedding_dim = 100\n",
            "2020-10-20 05:24:27,969 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.num_embeddings = None\n",
            "2020-10-20 05:24:27,969 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.projection_dim = None\n",
            "2020-10-20 05:24:27,969 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.weight = None\n",
            "2020-10-20 05:24:27,969 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.padding_index = 0\n",
            "2020-10-20 05:24:27,969 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.trainable = True\n",
            "2020-10-20 05:24:27,969 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.max_norm = None\n",
            "2020-10-20 05:24:27,969 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.norm_type = 2.0\n",
            "2020-10-20 05:24:27,969 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.scale_grad_by_freq = False\n",
            "2020-10-20 05:24:27,969 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.sparse = False\n",
            "2020-10-20 05:24:27,969 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.vocab_namespace = tokens\n",
            "2020-10-20 05:24:27,969 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.pretrained_file = None\n",
            "2020-10-20 05:24:28,106 - INFO - allennlp.common.params - model.seq2vec_encoder.type = cnn\n",
            "2020-10-20 05:24:28,106 - INFO - allennlp.common.params - model.seq2vec_encoder.embedding_dim = 100\n",
            "2020-10-20 05:24:28,106 - INFO - allennlp.common.params - model.seq2vec_encoder.num_filters = 64\n",
            "2020-10-20 05:24:28,106 - INFO - allennlp.common.params - model.seq2vec_encoder.ngram_filter_sizes = [2]\n",
            "2020-10-20 05:24:28,107 - INFO - allennlp.common.params - model.seq2vec_encoder.conv_layer_activation = None\n",
            "2020-10-20 05:24:28,107 - INFO - allennlp.common.params - model.seq2vec_encoder.output_dim = None\n",
            "2020-10-20 05:24:28,114 - INFO - allennlp.common.params - model.seq2seq_encoder = None\n",
            "2020-10-20 05:24:28,114 - INFO - allennlp.common.params - model.feedforward = None\n",
            "2020-10-20 05:24:28,114 - INFO - allennlp.common.params - model.dropout = None\n",
            "2020-10-20 05:24:28,114 - INFO - allennlp.common.params - model.num_labels = None\n",
            "2020-10-20 05:24:28,114 - INFO - allennlp.common.params - model.label_namespace = labels\n",
            "2020-10-20 05:24:28,114 - INFO - allennlp.common.params - model.namespace = tokens\n",
            "2020-10-20 05:24:28,114 - INFO - allennlp.common.params - model.initializer = <allennlp.nn.initializers.InitializerApplicator object at 0x7f9ad0be3a20>\n",
            "2020-10-20 05:24:28,115 - INFO - allennlp.nn.initializers - Initializing parameters\n",
            "2020-10-20 05:24:28,115 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code\n",
            "2020-10-20 05:24:28,115 - INFO - allennlp.nn.initializers -    _classification_layer.bias\n",
            "2020-10-20 05:24:28,115 - INFO - allennlp.nn.initializers -    _classification_layer.weight\n",
            "2020-10-20 05:24:28,115 - INFO - allennlp.nn.initializers -    _seq2vec_encoder.conv_layer_0.bias\n",
            "2020-10-20 05:24:28,115 - INFO - allennlp.nn.initializers -    _seq2vec_encoder.conv_layer_0.weight\n",
            "2020-10-20 05:24:28,115 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.weight\n",
            "2020-10-20 05:24:28,349 - INFO - allennlp.common.params - data_loader.type = pytorch_dataloader\n",
            "2020-10-20 05:24:28,349 - INFO - allennlp.common.params - data_loader.batch_size = 32\n",
            "2020-10-20 05:24:28,349 - INFO - allennlp.common.params - data_loader.shuffle = True\n",
            "2020-10-20 05:24:28,349 - INFO - allennlp.common.params - data_loader.sampler = None\n",
            "2020-10-20 05:24:28,349 - INFO - allennlp.common.params - data_loader.batch_sampler = None\n",
            "2020-10-20 05:24:28,349 - INFO - allennlp.common.params - data_loader.num_workers = 0\n",
            "2020-10-20 05:24:28,350 - INFO - allennlp.common.params - data_loader.pin_memory = False\n",
            "2020-10-20 05:24:28,350 - INFO - allennlp.common.params - data_loader.drop_last = False\n",
            "2020-10-20 05:24:28,350 - INFO - allennlp.common.params - data_loader.timeout = 0\n",
            "2020-10-20 05:24:28,350 - INFO - allennlp.common.params - data_loader.worker_init_fn = None\n",
            "2020-10-20 05:24:28,350 - INFO - allennlp.common.params - data_loader.multiprocessing_context = None\n",
            "2020-10-20 05:24:28,350 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None\n",
            "2020-10-20 05:24:28,356 - INFO - allennlp.common.params - validation_data_loader.type = pytorch_dataloader\n",
            "2020-10-20 05:24:28,357 - INFO - allennlp.common.params - validation_data_loader.batch_size = 32\n",
            "2020-10-20 05:24:28,357 - INFO - allennlp.common.params - validation_data_loader.shuffle = False\n",
            "2020-10-20 05:24:28,357 - INFO - allennlp.common.params - validation_data_loader.sampler = None\n",
            "2020-10-20 05:24:28,357 - INFO - allennlp.common.params - validation_data_loader.batch_sampler = None\n",
            "2020-10-20 05:24:28,357 - INFO - allennlp.common.params - validation_data_loader.num_workers = 0\n",
            "2020-10-20 05:24:28,357 - INFO - allennlp.common.params - validation_data_loader.pin_memory = False\n",
            "2020-10-20 05:24:28,357 - INFO - allennlp.common.params - validation_data_loader.drop_last = False\n",
            "2020-10-20 05:24:28,357 - INFO - allennlp.common.params - validation_data_loader.timeout = 0\n",
            "2020-10-20 05:24:28,358 - INFO - allennlp.common.params - validation_data_loader.worker_init_fn = None\n",
            "2020-10-20 05:24:28,358 - INFO - allennlp.common.params - validation_data_loader.multiprocessing_context = None\n",
            "2020-10-20 05:24:28,358 - INFO - allennlp.common.params - validation_data_loader.batches_per_epoch = None\n",
            "2020-10-20 05:24:28,358 - INFO - allennlp.common.params - trainer.type = gradient_descent\n",
            "2020-10-20 05:24:28,358 - INFO - allennlp.common.params - trainer.patience = 3\n",
            "2020-10-20 05:24:28,359 - INFO - allennlp.common.params - trainer.validation_metric = -loss\n",
            "2020-10-20 05:24:28,359 - INFO - allennlp.common.params - trainer.num_epochs = 10\n",
            "2020-10-20 05:24:28,359 - INFO - allennlp.common.params - trainer.cuda_device = 0\n",
            "2020-10-20 05:24:28,359 - INFO - allennlp.common.params - trainer.grad_norm = None\n",
            "2020-10-20 05:24:28,359 - INFO - allennlp.common.params - trainer.grad_clipping = None\n",
            "2020-10-20 05:24:28,359 - INFO - allennlp.common.params - trainer.distributed = None\n",
            "2020-10-20 05:24:28,359 - INFO - allennlp.common.params - trainer.world_size = 1\n",
            "2020-10-20 05:24:28,359 - INFO - allennlp.common.params - trainer.num_gradient_accumulation_steps = 1\n",
            "2020-10-20 05:24:28,359 - INFO - allennlp.common.params - trainer.use_amp = False\n",
            "2020-10-20 05:24:28,359 - INFO - allennlp.common.params - trainer.no_grad = None\n",
            "2020-10-20 05:24:28,360 - INFO - allennlp.common.params - trainer.learning_rate_scheduler = None\n",
            "2020-10-20 05:24:28,360 - INFO - allennlp.common.params - trainer.momentum_scheduler = None\n",
            "2020-10-20 05:24:28,360 - INFO - allennlp.common.params - trainer.tensorboard_writer = None\n",
            "2020-10-20 05:24:28,360 - INFO - allennlp.common.params - trainer.moving_average = None\n",
            "2020-10-20 05:24:28,360 - INFO - allennlp.common.params - trainer.checkpointer = None\n",
            "2020-10-20 05:24:28,360 - INFO - allennlp.common.params - trainer.batch_callbacks = None\n",
            "2020-10-20 05:24:28,360 - INFO - allennlp.common.params - trainer.epoch_callbacks = None\n",
            "2020-10-20 05:24:37,631 - INFO - allennlp.common.params - trainer.optimizer.type = adam\n",
            "2020-10-20 05:24:37,632 - INFO - allennlp.common.params - trainer.optimizer.parameter_groups = None\n",
            "2020-10-20 05:24:37,632 - INFO - allennlp.common.params - trainer.optimizer.lr = 0.001\n",
            "2020-10-20 05:24:37,632 - INFO - allennlp.common.params - trainer.optimizer.betas = (0.9, 0.999)\n",
            "2020-10-20 05:24:37,632 - INFO - allennlp.common.params - trainer.optimizer.eps = 1e-08\n",
            "2020-10-20 05:24:37,632 - INFO - allennlp.common.params - trainer.optimizer.weight_decay = 0.0\n",
            "2020-10-20 05:24:37,632 - INFO - allennlp.common.params - trainer.optimizer.amsgrad = False\n",
            "2020-10-20 05:24:37,632 - INFO - allennlp.training.optimizers - Number of trainable parameters: 11468394\n",
            "2020-10-20 05:24:37,633 - INFO - allennlp.common.util - The following parameters are Frozen (without gradient):\n",
            "2020-10-20 05:24:37,633 - INFO - allennlp.common.util - The following parameters are Tunable (with gradient):\n",
            "2020-10-20 05:24:37,633 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.weight\n",
            "2020-10-20 05:24:37,633 - INFO - allennlp.common.util - _seq2vec_encoder.conv_layer_0.weight\n",
            "2020-10-20 05:24:37,633 - INFO - allennlp.common.util - _seq2vec_encoder.conv_layer_0.bias\n",
            "2020-10-20 05:24:37,633 - INFO - allennlp.common.util - _classification_layer.weight\n",
            "2020-10-20 05:24:37,633 - INFO - allennlp.common.util - _classification_layer.bias\n",
            "2020-10-20 05:24:37,638 - INFO - allennlp.training.trainer - Beginning training.\n",
            "2020-10-20 05:24:37,638 - INFO - allennlp.training.trainer - Epoch 0/9\n",
            "2020-10-20 05:24:37,638 - INFO - allennlp.training.trainer - Worker 0 memory usage MB: 5249.244\n",
            "2020-10-20 05:24:37,733 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 835\n",
            "2020-10-20 05:24:37,733 - INFO - allennlp.training.trainer - Training\n",
            "accuracy: 0.8952, batch_loss: 0.1953, loss: 0.2704 ||: 100%|##########| 1250/1250 [00:14<00:00, 87.14it/s]\n",
            "2020-10-20 05:24:52,078 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.9162, batch_loss: 0.0537, loss: 0.2047 ||: 100%|##########| 157/157 [00:01<00:00, 140.10it/s]\n",
            "2020-10-20 05:24:53,199 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-10-20 05:24:53,200 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.895  |     0.916\n",
            "2020-10-20 05:24:53,205 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |   835.000  |       N/A\n",
            "2020-10-20 05:24:53,206 - INFO - allennlp.training.tensorboard_writer - loss               |     0.270  |     0.205\n",
            "2020-10-20 05:24:53,207 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5249.244  |       N/A\n",
            "2020-10-20 05:24:53,669 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to 'exp_amazon_reviews/best.th'.\n",
            "2020-10-20 05:24:53,743 - INFO - allennlp.training.trainer - Epoch duration: 0:00:16.105298\n",
            "2020-10-20 05:24:53,745 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:02:24\n",
            "2020-10-20 05:24:53,745 - INFO - allennlp.training.trainer - Epoch 1/9\n",
            "2020-10-20 05:24:53,745 - INFO - allennlp.training.trainer - Worker 0 memory usage MB: 5401.888\n",
            "2020-10-20 05:24:53,847 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 1649\n",
            "2020-10-20 05:24:53,848 - INFO - allennlp.training.trainer - Training\n",
            "accuracy: 0.9327, batch_loss: 0.2447, loss: 0.1718 ||: 100%|##########| 1250/1250 [00:08<00:00, 155.83it/s]\n",
            "2020-10-20 05:25:01,871 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.9216, batch_loss: 0.0457, loss: 0.1944 ||: 100%|##########| 157/157 [00:00<00:00, 391.86it/s]\n",
            "2020-10-20 05:25:02,273 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-10-20 05:25:02,274 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.933  |     0.922\n",
            "2020-10-20 05:25:02,275 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  1649.000  |       N/A\n",
            "2020-10-20 05:25:02,275 - INFO - allennlp.training.tensorboard_writer - loss               |     0.172  |     0.194\n",
            "2020-10-20 05:25:02,276 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5401.888  |       N/A\n",
            "2020-10-20 05:25:02,672 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to 'exp_amazon_reviews/best.th'.\n",
            "2020-10-20 05:25:02,750 - INFO - allennlp.training.trainer - Epoch duration: 0:00:09.004594\n",
            "2020-10-20 05:25:02,750 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:01:40\n",
            "2020-10-20 05:25:02,750 - INFO - allennlp.training.trainer - Epoch 2/9\n",
            "2020-10-20 05:25:02,750 - INFO - allennlp.training.trainer - Worker 0 memory usage MB: 5405.56\n",
            "2020-10-20 05:25:02,850 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 1649\n",
            "2020-10-20 05:25:02,851 - INFO - allennlp.training.trainer - Training\n",
            "accuracy: 0.9557, batch_loss: 0.1995, loss: 0.1148 ||: 100%|##########| 1250/1250 [00:08<00:00, 155.14it/s]\n",
            "2020-10-20 05:25:10,909 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.9230, batch_loss: 0.0195, loss: 0.2063 ||: 100%|##########| 157/157 [00:00<00:00, 385.14it/s]\n",
            "2020-10-20 05:25:11,317 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-10-20 05:25:11,318 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.956  |     0.923\n",
            "2020-10-20 05:25:11,318 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  1649.000  |       N/A\n",
            "2020-10-20 05:25:11,319 - INFO - allennlp.training.tensorboard_writer - loss               |     0.115  |     0.206\n",
            "2020-10-20 05:25:11,320 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5405.560  |       N/A\n",
            "2020-10-20 05:25:11,809 - INFO - allennlp.training.trainer - Epoch duration: 0:00:09.058518\n",
            "2020-10-20 05:25:11,809 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:01:19\n",
            "2020-10-20 05:25:11,809 - INFO - allennlp.training.trainer - Epoch 3/9\n",
            "2020-10-20 05:25:11,809 - INFO - allennlp.training.trainer - Worker 0 memory usage MB: 5407.48\n",
            "2020-10-20 05:25:11,918 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 1649\n",
            "2020-10-20 05:25:11,918 - INFO - allennlp.training.trainer - Training\n",
            "accuracy: 0.9765, batch_loss: 0.0608, loss: 0.0669 ||: 100%|##########| 1250/1250 [00:08<00:00, 155.00it/s]\n",
            "2020-10-20 05:25:19,984 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.9192, batch_loss: 0.0041, loss: 0.2290 ||: 100%|##########| 157/157 [00:00<00:00, 379.73it/s]\n",
            "2020-10-20 05:25:20,399 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-10-20 05:25:20,400 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.977  |     0.919\n",
            "2020-10-20 05:25:20,400 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  1649.000  |       N/A\n",
            "2020-10-20 05:25:20,401 - INFO - allennlp.training.tensorboard_writer - loss               |     0.067  |     0.229\n",
            "2020-10-20 05:25:20,402 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5407.480  |       N/A\n",
            "2020-10-20 05:25:20,831 - INFO - allennlp.training.trainer - Epoch duration: 0:00:09.022040\n",
            "2020-10-20 05:25:20,831 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:01:04\n",
            "2020-10-20 05:25:20,831 - INFO - allennlp.training.trainer - Epoch 4/9\n",
            "2020-10-20 05:25:20,832 - INFO - allennlp.training.trainer - Worker 0 memory usage MB: 5408.252\n",
            "2020-10-20 05:25:20,929 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 1649\n",
            "2020-10-20 05:25:20,929 - INFO - allennlp.training.trainer - Training\n",
            "accuracy: 0.9899, batch_loss: 0.0198, loss: 0.0327 ||: 100%|##########| 1250/1250 [00:08<00:00, 154.49it/s]\n",
            "2020-10-20 05:25:29,021 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.9112, batch_loss: 0.0045, loss: 0.2785 ||: 100%|##########| 157/157 [00:00<00:00, 394.28it/s]\n",
            "2020-10-20 05:25:29,420 - INFO - allennlp.training.trainer - Ran out of patience.  Stopping training.\n",
            "2020-10-20 05:25:29,421 - INFO - allennlp.training.checkpointer - loading best weights\n",
            "2020-10-20 05:25:29,455 - INFO - allennlp.common.util - Metrics: {\n",
            "  \"best_epoch\": 1,\n",
            "  \"peak_worker_0_memory_MB\": 5408.252,\n",
            "  \"peak_gpu_0_memory_MB\": 1649,\n",
            "  \"training_duration\": \"0:00:42.764338\",\n",
            "  \"training_start_epoch\": 0,\n",
            "  \"training_epochs\": 3,\n",
            "  \"epoch\": 3,\n",
            "  \"training_accuracy\": 0.976525,\n",
            "  \"training_loss\": 0.06688363437848166,\n",
            "  \"training_worker_0_memory_MB\": 5407.48,\n",
            "  \"training_gpu_0_memory_MB\": 1649,\n",
            "  \"validation_accuracy\": 0.9192,\n",
            "  \"validation_loss\": 0.22903894984821796,\n",
            "  \"best_validation_accuracy\": 0.9216,\n",
            "  \"best_validation_loss\": 0.1944208312779665\n",
            "}\n",
            "2020-10-20 05:25:29,455 - INFO - allennlp.models.archival - archiving weights and vocabulary to exp_amazon_reviews/model.tar.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQM_QHtqJgTE"
      },
      "source": [
        "検証データセットでの性能が良かったのは2エポック終了時のモデル（`\"best_epoch\": 1`）で、検証データセットを92.16% （`\"best_validation_accuracy\": 0.9216` ）の正解率で分類できたことが分かります。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EaYGoc_JsHF"
      },
      "source": [
        "## 性能の評価"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qi67ETIh46DW"
      },
      "source": [
        "`allennlp evaluate`コマンドを使って、テスト用データセットでの性能の評価を行います。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpbHZ3ARkK-A",
        "outputId": "87c38a9b-2f3e-4fd4-b7b3-aa1f22796c24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        " !allennlp evaluate --include-package mecab_tokenizer exp_amazon_reviews/model.tar.gz data/amazon_reviews/amazon_reviews_test.jsonl"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-10-20 05:25:39.069199: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-10-20 05:25:40,341 - INFO - transformers.file_utils - TensorFlow version 2.3.0 available.\n",
            "2020-10-20 05:25:41,122 - INFO - allennlp.models.archival - loading archive file exp_amazon_reviews/model.tar.gz\n",
            "2020-10-20 05:25:41,122 - INFO - allennlp.models.archival - extracting archive file exp_amazon_reviews/model.tar.gz to temp dir /tmp/tmptk66qql5\n",
            "2020-10-20 05:25:41,532 - INFO - allennlp.data.vocabulary - Loading token dictionary from /tmp/tmptk66qql5/vocabulary.\n",
            "2020-10-20 05:25:41,787 - INFO - allennlp.common.checks - Pytorch version: 1.6.0+cu101\n",
            "2020-10-20 05:25:41,789 - INFO - allennlp.commands.evaluate - Reading evaluation data from data/amazon_reviews/amazon_reviews_test.jsonl\n",
            "reading instances: 5000it [00:04, 1155.61it/s]\n",
            "2020-10-20 05:25:46,117 - INFO - allennlp.training.util - Iterating over dataset\n",
            "accuracy: 0.93, loss: 0.20 ||: : 157it [00:06, 26.07it/s]\n",
            "2020-10-20 05:25:52,140 - INFO - allennlp.commands.evaluate - Finished evaluating.\n",
            "2020-10-20 05:25:52,140 - INFO - allennlp.common.util - Metrics: {\n",
            "  \"accuracy\": 0.9296,\n",
            "  \"loss\": 0.19722449434268627\n",
            "}\n",
            "2020-10-20 05:25:52,280 - INFO - allennlp.models.archival - removing temporary unarchived model dir at /tmp/tmptk66qql5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sts760oyJ8ME"
      },
      "source": [
        "テスト用データセットでの正解率は92.96%になりました。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nr1sJic9J--Q"
      },
      "source": [
        "## 学習したモデルを使う"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38Iu946XNS6n"
      },
      "source": [
        "学習したモデルを使って感情分析を実行します。\n",
        "\n",
        "入力ファイル`sentiment_analysis_input.json`を作成し、`allennlp predict`コマンドで推論を実行します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prYESWt1KOM4",
        "outputId": "ff7d7288-2bf5-4b1c-82f4-53d6fbe8a88b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!echo '{\"sentence\":\"この本は、役に立つし、面白い。\"}' > sentiment_analysis_input.json\n",
        "!allennlp predict --include-package mecab_tokenizer exp_amazon_reviews/model.tar.gz sentiment_analysis_input.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-10-20 05:25:54.391405: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-10-20 05:25:55,683 - INFO - transformers.file_utils - TensorFlow version 2.3.0 available.\n",
            "2020-10-20 05:25:56,476 - INFO - allennlp.models.archival - loading archive file exp_amazon_reviews/model.tar.gz\n",
            "2020-10-20 05:25:56,476 - INFO - allennlp.models.archival - extracting archive file exp_amazon_reviews/model.tar.gz to temp dir /tmp/tmpz4w6tibk\n",
            "2020-10-20 05:25:56,876 - INFO - allennlp.common.params - vocabulary.type = from_instances\n",
            "2020-10-20 05:25:56,876 - INFO - allennlp.data.vocabulary - Loading token dictionary from /tmp/tmpz4w6tibk/vocabulary.\n",
            "2020-10-20 05:25:56,991 - INFO - allennlp.common.params - model.type = basic_classifier\n",
            "2020-10-20 05:25:56,991 - INFO - allennlp.common.params - model.regularizer = None\n",
            "2020-10-20 05:25:56,992 - INFO - allennlp.common.params - model.text_field_embedder.type = basic\n",
            "2020-10-20 05:25:56,992 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.type = embedding\n",
            "2020-10-20 05:25:56,992 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.embedding_dim = 100\n",
            "2020-10-20 05:25:56,992 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.num_embeddings = None\n",
            "2020-10-20 05:25:56,992 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.projection_dim = None\n",
            "2020-10-20 05:25:56,992 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.weight = None\n",
            "2020-10-20 05:25:56,992 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.padding_index = 0\n",
            "2020-10-20 05:25:56,992 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.trainable = True\n",
            "2020-10-20 05:25:56,992 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.max_norm = None\n",
            "2020-10-20 05:25:56,993 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.norm_type = 2.0\n",
            "2020-10-20 05:25:56,993 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.scale_grad_by_freq = False\n",
            "2020-10-20 05:25:56,993 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.sparse = False\n",
            "2020-10-20 05:25:56,993 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.vocab_namespace = tokens\n",
            "2020-10-20 05:25:56,993 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.pretrained_file = None\n",
            "2020-10-20 05:25:57,080 - INFO - allennlp.common.params - model.seq2vec_encoder.type = cnn\n",
            "2020-10-20 05:25:57,080 - INFO - allennlp.common.params - model.seq2vec_encoder.embedding_dim = 100\n",
            "2020-10-20 05:25:57,080 - INFO - allennlp.common.params - model.seq2vec_encoder.num_filters = 64\n",
            "2020-10-20 05:25:57,080 - INFO - allennlp.common.params - model.seq2vec_encoder.ngram_filter_sizes = [2]\n",
            "2020-10-20 05:25:57,080 - INFO - allennlp.common.params - model.seq2vec_encoder.conv_layer_activation = None\n",
            "2020-10-20 05:25:57,080 - INFO - allennlp.common.params - model.seq2vec_encoder.output_dim = None\n",
            "2020-10-20 05:25:57,083 - INFO - allennlp.common.params - model.seq2seq_encoder = None\n",
            "2020-10-20 05:25:57,084 - INFO - allennlp.common.params - model.feedforward = None\n",
            "2020-10-20 05:25:57,084 - INFO - allennlp.common.params - model.dropout = None\n",
            "2020-10-20 05:25:57,084 - INFO - allennlp.common.params - model.num_labels = None\n",
            "2020-10-20 05:25:57,084 - INFO - allennlp.common.params - model.label_namespace = labels\n",
            "2020-10-20 05:25:57,084 - INFO - allennlp.common.params - model.namespace = tokens\n",
            "2020-10-20 05:25:57,084 - INFO - allennlp.common.params - model.initializer = <allennlp.nn.initializers.InitializerApplicator object at 0x7fdd700139e8>\n",
            "2020-10-20 05:25:57,084 - INFO - allennlp.nn.initializers - Initializing parameters\n",
            "2020-10-20 05:25:57,084 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code\n",
            "2020-10-20 05:25:57,084 - INFO - allennlp.nn.initializers -    _classification_layer.bias\n",
            "2020-10-20 05:25:57,084 - INFO - allennlp.nn.initializers -    _classification_layer.weight\n",
            "2020-10-20 05:25:57,085 - INFO - allennlp.nn.initializers -    _seq2vec_encoder.conv_layer_0.bias\n",
            "2020-10-20 05:25:57,085 - INFO - allennlp.nn.initializers -    _seq2vec_encoder.conv_layer_0.weight\n",
            "2020-10-20 05:25:57,085 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.weight\n",
            "2020-10-20 05:25:57,120 - INFO - allennlp.common.params - dataset_reader.type = text_classification_json\n",
            "2020-10-20 05:25:57,121 - INFO - allennlp.common.params - dataset_reader.lazy = False\n",
            "2020-10-20 05:25:57,121 - INFO - allennlp.common.params - dataset_reader.cache_directory = None\n",
            "2020-10-20 05:25:57,121 - INFO - allennlp.common.params - dataset_reader.max_instances = None\n",
            "2020-10-20 05:25:57,121 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False\n",
            "2020-10-20 05:25:57,121 - INFO - allennlp.common.params - dataset_reader.manual_multi_process_sharding = False\n",
            "2020-10-20 05:25:57,121 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = single_id\n",
            "2020-10-20 05:25:57,121 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.namespace = tokens\n",
            "2020-10-20 05:25:57,121 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.lowercase_tokens = False\n",
            "2020-10-20 05:25:57,122 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.start_tokens = None\n",
            "2020-10-20 05:25:57,122 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.end_tokens = None\n",
            "2020-10-20 05:25:57,122 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.feature_name = text\n",
            "2020-10-20 05:25:57,122 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.default_value = THIS IS A REALLY UNLIKELY VALUE THAT HAS TO BE A STRING\n",
            "2020-10-20 05:25:57,122 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
            "2020-10-20 05:25:57,122 - INFO - allennlp.common.params - dataset_reader.tokenizer.type = mecab\n",
            "2020-10-20 05:25:57,123 - INFO - allennlp.common.params - dataset_reader.segment_sentences = False\n",
            "2020-10-20 05:25:57,123 - INFO - allennlp.common.params - dataset_reader.max_sequence_length = None\n",
            "2020-10-20 05:25:57,123 - INFO - allennlp.common.params - dataset_reader.skip_label_indexing = False\n",
            "input 0:  {\"sentence\": \"\\u3053\\u306e\\u672c\\u306f\\u3001\\u5f79\\u306b\\u7acb\\u3064\\u3057\\u3001\\u9762\\u767d\\u3044\\u3002\"}\n",
            "prediction:  {\"logits\": [3.387394666671753, -2.742903470993042], \"probs\": [0.9978287816047668, 0.0021712076850235462], \"token_ids\": [28, 91, 7, 3, 9574, 15, 3, 270, 4], \"label\": \"positive\", \"tokens\": [\"\\u3053\\u306e\", \"\\u672c\", \"\\u306f\", \"\\u3001\", \"\\u5f79\\u306b\\u7acb\\u3064\", \"\\u3057\", \"\\u3001\", \"\\u9762\\u767d\\u3044\", \"\\u3002\"]}\n",
            "\n",
            "2020-10-20 05:25:57,137 - INFO - allennlp.models.archival - removing temporary unarchived model dir at /tmp/tmpz4w6tibk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AHt8p2LKy3o"
      },
      "source": [
        "prediction:からはじまる行を整形した結果を示します。 \n",
        "\n",
        "正しくpositiveに分類されています。\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"logits\": [3.387394666671753, -2.742903470993042], \n",
        "  \"probs\": [0.9978287816047668, 0.0021712076850235462], \n",
        "  \"token_ids\": [28, 91, 7, 3, 9574, 15, 3, 270, 4], \n",
        "  \"label\": \"positive\", \n",
        "  \"tokens\": [\"\\u3053\\u306e\", \"\\u672c\", \"\\u306f\", \"\\u3001\", \"\\u5f79\\u306b\\u7acb\\u3064\", \"\\u3057\", \"\\u3001\", \"\\u9762\\u767d\\u3044\", \"\\u3002\"]\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTRVDBRcKWSQ",
        "outputId": "1486feb4-f85c-4bb5-f2b8-98906e9393d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!echo '{\"sentence\":\"この本は、役に立たないし、面白くない。\"}' > sentiment_analysis_input.json\n",
        "!allennlp predict --include-package mecab_tokenizer exp_amazon_reviews/model.tar.gz sentiment_analysis_input.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-10-20 05:25:59.181067: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-10-20 05:26:00,445 - INFO - transformers.file_utils - TensorFlow version 2.3.0 available.\n",
            "2020-10-20 05:26:01,242 - INFO - allennlp.models.archival - loading archive file exp_amazon_reviews/model.tar.gz\n",
            "2020-10-20 05:26:01,242 - INFO - allennlp.models.archival - extracting archive file exp_amazon_reviews/model.tar.gz to temp dir /tmp/tmp5fgtvnh_\n",
            "2020-10-20 05:26:01,645 - INFO - allennlp.common.params - vocabulary.type = from_instances\n",
            "2020-10-20 05:26:01,645 - INFO - allennlp.data.vocabulary - Loading token dictionary from /tmp/tmp5fgtvnh_/vocabulary.\n",
            "2020-10-20 05:26:01,764 - INFO - allennlp.common.params - model.type = basic_classifier\n",
            "2020-10-20 05:26:01,764 - INFO - allennlp.common.params - model.regularizer = None\n",
            "2020-10-20 05:26:01,765 - INFO - allennlp.common.params - model.text_field_embedder.type = basic\n",
            "2020-10-20 05:26:01,765 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.type = embedding\n",
            "2020-10-20 05:26:01,765 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.embedding_dim = 100\n",
            "2020-10-20 05:26:01,765 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.num_embeddings = None\n",
            "2020-10-20 05:26:01,765 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.projection_dim = None\n",
            "2020-10-20 05:26:01,765 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.weight = None\n",
            "2020-10-20 05:26:01,765 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.padding_index = 0\n",
            "2020-10-20 05:26:01,766 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.trainable = True\n",
            "2020-10-20 05:26:01,766 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.max_norm = None\n",
            "2020-10-20 05:26:01,766 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.norm_type = 2.0\n",
            "2020-10-20 05:26:01,766 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.scale_grad_by_freq = False\n",
            "2020-10-20 05:26:01,766 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.sparse = False\n",
            "2020-10-20 05:26:01,766 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.vocab_namespace = tokens\n",
            "2020-10-20 05:26:01,766 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.pretrained_file = None\n",
            "2020-10-20 05:26:01,851 - INFO - allennlp.common.params - model.seq2vec_encoder.type = cnn\n",
            "2020-10-20 05:26:01,852 - INFO - allennlp.common.params - model.seq2vec_encoder.embedding_dim = 100\n",
            "2020-10-20 05:26:01,852 - INFO - allennlp.common.params - model.seq2vec_encoder.num_filters = 64\n",
            "2020-10-20 05:26:01,852 - INFO - allennlp.common.params - model.seq2vec_encoder.ngram_filter_sizes = [2]\n",
            "2020-10-20 05:26:01,852 - INFO - allennlp.common.params - model.seq2vec_encoder.conv_layer_activation = None\n",
            "2020-10-20 05:26:01,852 - INFO - allennlp.common.params - model.seq2vec_encoder.output_dim = None\n",
            "2020-10-20 05:26:01,857 - INFO - allennlp.common.params - model.seq2seq_encoder = None\n",
            "2020-10-20 05:26:01,858 - INFO - allennlp.common.params - model.feedforward = None\n",
            "2020-10-20 05:26:01,858 - INFO - allennlp.common.params - model.dropout = None\n",
            "2020-10-20 05:26:01,858 - INFO - allennlp.common.params - model.num_labels = None\n",
            "2020-10-20 05:26:01,858 - INFO - allennlp.common.params - model.label_namespace = labels\n",
            "2020-10-20 05:26:01,858 - INFO - allennlp.common.params - model.namespace = tokens\n",
            "2020-10-20 05:26:01,858 - INFO - allennlp.common.params - model.initializer = <allennlp.nn.initializers.InitializerApplicator object at 0x7f81f029d9e8>\n",
            "2020-10-20 05:26:01,858 - INFO - allennlp.nn.initializers - Initializing parameters\n",
            "2020-10-20 05:26:01,859 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code\n",
            "2020-10-20 05:26:01,859 - INFO - allennlp.nn.initializers -    _classification_layer.bias\n",
            "2020-10-20 05:26:01,859 - INFO - allennlp.nn.initializers -    _classification_layer.weight\n",
            "2020-10-20 05:26:01,859 - INFO - allennlp.nn.initializers -    _seq2vec_encoder.conv_layer_0.bias\n",
            "2020-10-20 05:26:01,859 - INFO - allennlp.nn.initializers -    _seq2vec_encoder.conv_layer_0.weight\n",
            "2020-10-20 05:26:01,859 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.weight\n",
            "2020-10-20 05:26:01,897 - INFO - allennlp.common.params - dataset_reader.type = text_classification_json\n",
            "2020-10-20 05:26:01,898 - INFO - allennlp.common.params - dataset_reader.lazy = False\n",
            "2020-10-20 05:26:01,898 - INFO - allennlp.common.params - dataset_reader.cache_directory = None\n",
            "2020-10-20 05:26:01,898 - INFO - allennlp.common.params - dataset_reader.max_instances = None\n",
            "2020-10-20 05:26:01,898 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False\n",
            "2020-10-20 05:26:01,898 - INFO - allennlp.common.params - dataset_reader.manual_multi_process_sharding = False\n",
            "2020-10-20 05:26:01,899 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = single_id\n",
            "2020-10-20 05:26:01,899 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.namespace = tokens\n",
            "2020-10-20 05:26:01,899 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.lowercase_tokens = False\n",
            "2020-10-20 05:26:01,899 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.start_tokens = None\n",
            "2020-10-20 05:26:01,899 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.end_tokens = None\n",
            "2020-10-20 05:26:01,899 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.feature_name = text\n",
            "2020-10-20 05:26:01,899 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.default_value = THIS IS A REALLY UNLIKELY VALUE THAT HAS TO BE A STRING\n",
            "2020-10-20 05:26:01,899 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
            "2020-10-20 05:26:01,899 - INFO - allennlp.common.params - dataset_reader.tokenizer.type = mecab\n",
            "2020-10-20 05:26:01,900 - INFO - allennlp.common.params - dataset_reader.segment_sentences = False\n",
            "2020-10-20 05:26:01,900 - INFO - allennlp.common.params - dataset_reader.max_sequence_length = None\n",
            "2020-10-20 05:26:01,900 - INFO - allennlp.common.params - dataset_reader.skip_label_indexing = False\n",
            "input 0:  {\"sentence\": \"\\u3053\\u306e\\u672c\\u306f\\u3001\\u5f79\\u306b\\u7acb\\u305f\\u306a\\u3044\\u3057\\u3001\\u9762\\u767d\\u304f\\u306a\\u3044\\u3002\"}\n",
            "prediction:  {\"logits\": [-0.8659035563468933, 1.2352415323257446], \"probs\": [0.10898558795452118, 0.8910144567489624], \"token_ids\": [28, 91, 7, 3, 22670, 18, 15, 3, 902, 18, 4], \"label\": \"negative\", \"tokens\": [\"\\u3053\\u306e\", \"\\u672c\", \"\\u306f\", \"\\u3001\", \"\\u5f79\\u306b\\u7acb\\u305f\", \"\\u306a\\u3044\", \"\\u3057\", \"\\u3001\", \"\\u9762\\u767d\\u304f\", \"\\u306a\\u3044\", \"\\u3002\"]}\n",
            "\n",
            "2020-10-20 05:26:01,911 - INFO - allennlp.models.archival - removing temporary unarchived model dir at /tmp/tmp5fgtvnh_\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMBOw-98LIU2"
      },
      "source": [
        "正しくnegative (`\"label\": \"negative\"`)となりました。\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"logits\": [-0.8659035563468933, 1.2352415323257446], \n",
        "  \"probs\": [0.10898558795452118, 0.8910144567489624], \n",
        "  \"token_ids\": [28, 91, 7, 3, 22670, 18, 15, 3, 902, 18, 4], \n",
        "  \"label\": \"negative\", \n",
        "  \"tokens\": [\"\\u3053\\u306e\", \"\\u672c\", \"\\u306f\", \"\\u3001\", \"\\u5f79\\u306b\\u7acb\\u305f\", \"\\u306a\\u3044\", \"\\u3057\", \"\\u3001\", \"\\u9762\\u767d\\u304f\", \"\\u306a\\u3044\", \"\\u3002\"]\n",
        "}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBhlXaYYKhSu",
        "outputId": "3955016c-98ce-4651-8fea-8c1cca4bb9b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!echo '{\"sentence\":\"この本は、役に立たないけど、面白い。\"}' > sentiment_analysis_input.json\n",
        "!allennlp predict --include-package mecab_tokenizer exp_amazon_reviews/model.tar.gz sentiment_analysis_input.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-10-20 05:26:04.055254: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-10-20 05:26:05,357 - INFO - transformers.file_utils - TensorFlow version 2.3.0 available.\n",
            "2020-10-20 05:26:06,137 - INFO - allennlp.models.archival - loading archive file exp_amazon_reviews/model.tar.gz\n",
            "2020-10-20 05:26:06,137 - INFO - allennlp.models.archival - extracting archive file exp_amazon_reviews/model.tar.gz to temp dir /tmp/tmpxdh45u81\n",
            "2020-10-20 05:26:06,532 - INFO - allennlp.common.params - vocabulary.type = from_instances\n",
            "2020-10-20 05:26:06,532 - INFO - allennlp.data.vocabulary - Loading token dictionary from /tmp/tmpxdh45u81/vocabulary.\n",
            "2020-10-20 05:26:06,653 - INFO - allennlp.common.params - model.type = basic_classifier\n",
            "2020-10-20 05:26:06,654 - INFO - allennlp.common.params - model.regularizer = None\n",
            "2020-10-20 05:26:06,654 - INFO - allennlp.common.params - model.text_field_embedder.type = basic\n",
            "2020-10-20 05:26:06,654 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.type = embedding\n",
            "2020-10-20 05:26:06,655 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.embedding_dim = 100\n",
            "2020-10-20 05:26:06,655 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.num_embeddings = None\n",
            "2020-10-20 05:26:06,655 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.projection_dim = None\n",
            "2020-10-20 05:26:06,655 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.weight = None\n",
            "2020-10-20 05:26:06,655 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.padding_index = 0\n",
            "2020-10-20 05:26:06,655 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.trainable = True\n",
            "2020-10-20 05:26:06,655 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.max_norm = None\n",
            "2020-10-20 05:26:06,655 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.norm_type = 2.0\n",
            "2020-10-20 05:26:06,655 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.scale_grad_by_freq = False\n",
            "2020-10-20 05:26:06,655 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.sparse = False\n",
            "2020-10-20 05:26:06,655 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.vocab_namespace = tokens\n",
            "2020-10-20 05:26:06,655 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.pretrained_file = None\n",
            "2020-10-20 05:26:06,738 - INFO - allennlp.common.params - model.seq2vec_encoder.type = cnn\n",
            "2020-10-20 05:26:06,738 - INFO - allennlp.common.params - model.seq2vec_encoder.embedding_dim = 100\n",
            "2020-10-20 05:26:06,738 - INFO - allennlp.common.params - model.seq2vec_encoder.num_filters = 64\n",
            "2020-10-20 05:26:06,738 - INFO - allennlp.common.params - model.seq2vec_encoder.ngram_filter_sizes = [2]\n",
            "2020-10-20 05:26:06,738 - INFO - allennlp.common.params - model.seq2vec_encoder.conv_layer_activation = None\n",
            "2020-10-20 05:26:06,739 - INFO - allennlp.common.params - model.seq2vec_encoder.output_dim = None\n",
            "2020-10-20 05:26:06,742 - INFO - allennlp.common.params - model.seq2seq_encoder = None\n",
            "2020-10-20 05:26:06,742 - INFO - allennlp.common.params - model.feedforward = None\n",
            "2020-10-20 05:26:06,742 - INFO - allennlp.common.params - model.dropout = None\n",
            "2020-10-20 05:26:06,742 - INFO - allennlp.common.params - model.num_labels = None\n",
            "2020-10-20 05:26:06,742 - INFO - allennlp.common.params - model.label_namespace = labels\n",
            "2020-10-20 05:26:06,742 - INFO - allennlp.common.params - model.namespace = tokens\n",
            "2020-10-20 05:26:06,742 - INFO - allennlp.common.params - model.initializer = <allennlp.nn.initializers.InitializerApplicator object at 0x7fa23c0769e8>\n",
            "2020-10-20 05:26:06,743 - INFO - allennlp.nn.initializers - Initializing parameters\n",
            "2020-10-20 05:26:06,743 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code\n",
            "2020-10-20 05:26:06,743 - INFO - allennlp.nn.initializers -    _classification_layer.bias\n",
            "2020-10-20 05:26:06,743 - INFO - allennlp.nn.initializers -    _classification_layer.weight\n",
            "2020-10-20 05:26:06,743 - INFO - allennlp.nn.initializers -    _seq2vec_encoder.conv_layer_0.bias\n",
            "2020-10-20 05:26:06,743 - INFO - allennlp.nn.initializers -    _seq2vec_encoder.conv_layer_0.weight\n",
            "2020-10-20 05:26:06,743 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.weight\n",
            "2020-10-20 05:26:06,781 - INFO - allennlp.common.params - dataset_reader.type = text_classification_json\n",
            "2020-10-20 05:26:06,781 - INFO - allennlp.common.params - dataset_reader.lazy = False\n",
            "2020-10-20 05:26:06,781 - INFO - allennlp.common.params - dataset_reader.cache_directory = None\n",
            "2020-10-20 05:26:06,781 - INFO - allennlp.common.params - dataset_reader.max_instances = None\n",
            "2020-10-20 05:26:06,781 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False\n",
            "2020-10-20 05:26:06,781 - INFO - allennlp.common.params - dataset_reader.manual_multi_process_sharding = False\n",
            "2020-10-20 05:26:06,782 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = single_id\n",
            "2020-10-20 05:26:06,782 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.namespace = tokens\n",
            "2020-10-20 05:26:06,782 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.lowercase_tokens = False\n",
            "2020-10-20 05:26:06,782 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.start_tokens = None\n",
            "2020-10-20 05:26:06,782 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.end_tokens = None\n",
            "2020-10-20 05:26:06,782 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.feature_name = text\n",
            "2020-10-20 05:26:06,782 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.default_value = THIS IS A REALLY UNLIKELY VALUE THAT HAS TO BE A STRING\n",
            "2020-10-20 05:26:06,782 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
            "2020-10-20 05:26:06,782 - INFO - allennlp.common.params - dataset_reader.tokenizer.type = mecab\n",
            "2020-10-20 05:26:06,783 - INFO - allennlp.common.params - dataset_reader.segment_sentences = False\n",
            "2020-10-20 05:26:06,783 - INFO - allennlp.common.params - dataset_reader.max_sequence_length = None\n",
            "2020-10-20 05:26:06,783 - INFO - allennlp.common.params - dataset_reader.skip_label_indexing = False\n",
            "input 0:  {\"sentence\": \"\\u3053\\u306e\\u672c\\u306f\\u3001\\u5f79\\u306b\\u7acb\\u305f\\u306a\\u3044\\u3051\\u3069\\u3001\\u9762\\u767d\\u3044\\u3002\"}\n",
            "prediction:  {\"logits\": [2.9268059730529785, -2.210545778274536], \"probs\": [0.994161069393158, 0.0058389282785356045], \"token_ids\": [28, 91, 7, 3, 22670, 18, 116, 3, 270, 4], \"label\": \"positive\", \"tokens\": [\"\\u3053\\u306e\", \"\\u672c\", \"\\u306f\", \"\\u3001\", \"\\u5f79\\u306b\\u7acb\\u305f\", \"\\u306a\\u3044\", \"\\u3051\\u3069\", \"\\u3001\", \"\\u9762\\u767d\\u3044\", \"\\u3002\"]}\n",
            "\n",
            "2020-10-20 05:26:06,794 - INFO - allennlp.models.archival - removing temporary unarchived model dir at /tmp/tmpxdh45u81\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOIPUw8ZLYke"
      },
      "source": [
        "こちらはpositive（`\"label\": \"positive\"`）に分類されました。\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"logits\": [2.9268059730529785, -2.210545778274536], \n",
        "  \"probs\": [0.994161069393158, 0.0058389282785356045], \n",
        "  \"token_ids\": [28, 91, 7, 3, 22670, 18, 116, 3, 270, 4], \n",
        "  \"label\": \"positive\", \n",
        "  \"tokens\": [\"\\u3053\\u306e\", \"\\u672c\", \"\\u306f\", \"\\u3001\", \"\\u5f79\\u306b\\u7acb\\u305f\", \"\\u306a\\u3044\", \"\\u3051\\u3069\", \"\\u3001\", \"\\u9762\\u767d\\u3044\", \"\\u3002\"]\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnGqg6-vmOBB"
      },
      "source": [
        "**他のノートブックへのリンク：**\n",
        "\n",
        "* [AllenNLPによる自然言語処理 (1): bag-of-embeddingsモデルによる文書分類](https://colab.research.google.com/drive/1yquPsCgv7EpNPheqt_Th-j-kK3CN2VWQ?usp=sharing)\n",
        "* [AllenNLPによる自然言語処理 (3): BERTによる固有表現認識](https://colab.research.google.com/drive/13ga1yYYZkosGZy9ZinAB76blb-8k6yby?usp=sharing)"
      ]
    }
  ]
}