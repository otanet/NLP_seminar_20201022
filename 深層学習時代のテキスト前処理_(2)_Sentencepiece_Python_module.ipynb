{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "深層学習時代のテキスト前処理:(2) Sentencepiece Python module",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/otanet/NLP_seminar_20201022/blob/main/%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92%E6%99%82%E4%BB%A3%E3%81%AE%E3%83%86%E3%82%AD%E3%82%B9%E3%83%88%E5%89%8D%E5%87%A6%E7%90%86_(2)_Sentencepiece_Python_module.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9BDzLVkUFT4"
      },
      "source": [
        "# Sentencepiece python モジュール\n",
        "\n",
        "このノートブックでは、Sentencepiece Pythonモジュールの使用方法を解説します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIgXb6P2Yg6g"
      },
      "source": [
        "## インストールと準備\n",
        "\n",
        "sentencepiece は、pip コマンド経由で簡単に導入することができます。\n",
        "\n",
        "このノートブックでは、小説[坊っちゃん](https://en.wikipedia.org/wiki/Botchan) の英訳を入力データのサンプルとして用います。 同データは sentencepiece のパッケージの中に同封されています。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUcAbKnRVAv6",
        "outputId": "a3c7726c-1d94-425e-e72d-1522960b18ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        }
      },
      "source": [
        "!pip install sentencepiece\n",
        "!wget https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.91)\n",
            "--2020-10-19 16:15:02--  https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 278779 (272K) [text/plain]\n",
            "Saving to: ‘botchan.txt.1’\n",
            "\n",
            "botchan.txt.1       100%[===================>] 272.25K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2020-10-19 16:15:03 (8.64 MB/s) - ‘botchan.txt.1’ saved [278779/278779]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-k5KbVgiYae-"
      },
      "source": [
        "## 基本的な動作方法\n",
        "\n",
        "###  サブワードの学習\n",
        "\n",
        "サブワードの学習は、**SentencePieceTrainer.train** static メソッドを用います。コマンドラインツール spm_train の引数を python の引数スタイルで与えます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-WIzSPKisKj"
      },
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "# 'botchan.txt' を用い学習し、'm.model', 'm.vocab' を生成\n",
        "# 'm.vocab' はデバッグ目的に出力され、分割時には不要\n",
        "spm.SentencePieceTrainer.train(input='botchan.txt', model_prefix='m', vocab_size=2000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMCT8KmXivw3"
      },
      "source": [
        "### トークン化 (エンコード)\n",
        "\n",
        "トークン化には、**SentencePieceProcessor** クラスを用います。コンストラクタにモデルファイルを渡し初期化します。\n",
        "\n",
        "**encode** メソッドでトークン化を行います。**out_type** を指定することで、トークン文字列あるいは id 列の出力を切り替えることができます。**encode** は文のリストも受け付けます。この場合は、結果のリストが返ります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ee9W6wGnVteW",
        "cellView": "code",
        "outputId": "5d148be0-ffea-4768-ba08-b1f539e2fab0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "sp = spm.SentencePieceProcessor(model_file='m.model')\n",
        "\n",
        "# encode: text => トークン列 あるいは id 列への変換\n",
        "print(sp.encode('This is a test', out_type=str))\n",
        "print(sp.encode('This is a test', out_type=int))\n",
        "\n",
        "# 複数文の処理\n",
        "print(sp.encode(['This is a test', 'hello world', 'I saw a girl with a telescope'],\n",
        "                out_type=int))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['▁This', '▁is', '▁a', '▁t', 'est']\n",
            "[212, 32, 10, 587, 446]\n",
            "[[212, 32, 10, 587, 446], [29, 134, 44, 1040], [6, 291, 89, 10, 1097, 26, 10, 9, 228, 126, 8, 82, 310, 20]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMMFDBxAjOoy"
      },
      "source": [
        "### 脱トークン化 (デコード)\n",
        "\n",
        "**decode** メソッドを用いて、id 列やトークン列から元の文を復元します。**encode** と同様、リスト入力を受け付けます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnFiUxOgjd8f",
        "outputId": "d8756dd5-d25e-4e0d-f32d-cb19f2c6bf76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# decode: id => text\n",
        "print(sp.decode(['▁This', '▁is', '▁a', '▁t', 'est']))\n",
        "print(sp.decode([212, 32, 10, 587, 446]))\n",
        "print(sp.decode([[212, 32, 10, 587, 446],[29, 134, 44, 1040]]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is a test\n",
            "This is a test\n",
            "['This is a test', 'hello world']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qt6xKwVpkIxy"
      },
      "source": [
        "### トークンとIDのマッピング\n",
        "\n",
        "**piece_size** を使い語彙数を取得することができます。\n",
        "\n",
        "**id_to_piece**, **piece_to_id** を用い、id <=> トークンの変換を行います。\n",
        "**is_control**は、 id が制御トークンかを返します。同様に、**is_unknown** は、未知文字かどうかを返します。\n",
        "\n",
        "これらのメソッドも同様に、リストを受け付けます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vHnQbBOltZo",
        "outputId": "655f06e1-df38-4be9-a691-75bbd2adc808",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        }
      },
      "source": [
        "# 語彙サイズ\n",
        "print(sp.piece_size())\n",
        "\n",
        "# id <=> piece 変換\n",
        "print(sp.id_to_piece(209))\n",
        "print(sp.piece_to_id('▁This'))\n",
        "print(sp.piece_to_id(['▁This', '▁is', '▁a', '▁t', 'est']))\n",
        "print(sp.id_to_piece([212, 32, 10, 587, 446]))\n",
        "\n",
        "# 未知語は id 0 が返る (変更可能)\n",
        "print(sp.piece_to_id('__MUST_BE_UNKNOWN__'))\n",
        "\n",
        "# <unk>, <s>, </s> の id の表示\n",
        "print(sp.id_to_piece([0,1,2]))\n",
        "print(sp.is_control([0,1,2]))\n",
        "print(sp.is_unknown([0,1,2]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2000\n",
            "ok\n",
            "212\n",
            "[212, 32, 10, 587, 446]\n",
            "['▁This', '▁is', '▁a', '▁t', 'est']\n",
            "0\n",
            "['<unk>', '<s>', '</s>']\n",
            "[False, True, True]\n",
            "[True, False, False]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRv6EeC2Y2PE"
      },
      "source": [
        "## 実ファイルに依存しないオンメモリ処理\n",
        "\n",
        "Sentencepiece の学習およびトークン化は、物理的なファイルに依存することなく、メモリ上で行うことが可能です。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2GLGk_71HQy"
      },
      "source": [
        "\n",
        "### ファイルに依存しない学習\n",
        "**SentencePieceTrainer.train()** は、**sentence_iterator** 引数に任意のイテラブルなオブジェクトを渡すことで、文の集合をイテレーターとしてフィードすることができます。\n",
        "さらに、**model_writer** に write メソッドを持つ任意のオブジェクトを渡すことで、\n",
        "モデルのbyte表現の書き込みを移譲することができます。\n",
        "\n",
        "以下は、文が格納された文字列 list から学習し、BytesIO にモデルを書き込む例です。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Bdi9SuxYAud"
      },
      "source": [
        "import io\n",
        "\n",
        "train = []\n",
        "with open('botchan.txt', 'r') as f:\n",
        "    for line in f:\n",
        "        line = line.rstrip()\n",
        "        train.append(line)\n",
        "\n",
        "model_proto = io.BytesIO()\n",
        "\n",
        "spm.SentencePieceTrainer.train(sentence_iterator=iter(train), \n",
        "                                model_writer=model_proto,\n",
        "                               vocab_size=2000)\n",
        "\n",
        "with open('spm_model.model', 'wb') as f:\n",
        "    f.write(model_proto.getvalue())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWLSNQ-eyPbE"
      },
      "source": [
        "イテラブルなオブジェクトであれば、どのような入力からも学習できます。以下は、外部サイトファイルから直接学習する例です。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0ODQqqSyO_U",
        "outputId": "252e62dc-08b7-478a-e607-ec7152d65d3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import urllib.request\n",
        "import io\n",
        "import sentencepiece as spm\n",
        "\n",
        "model = io.BytesIO()\n",
        "with urllib.request.urlopen(\n",
        "    'https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt'\n",
        ") as response:\n",
        "  spm.SentencePieceTrainer.train(\n",
        "      sentence_iterator=response, model_writer=model, vocab_size=2000)\n",
        "  \n",
        "  sp = spm.SentencePieceProcessor(model_proto=model.getvalue())\n",
        "  print(sp.encode('hello world', out_type=str))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['▁he', 'll', 'o', '▁world']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NMUG145nLUe"
      },
      "source": [
        "### ファイルに依存しないトークン化\n",
        "\n",
        "**model_proto** パラメータにモデルのバイト列を渡すことで、メモリ上のモデルから **SentencepieceProcessor** を初期化できます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBZuyZTvnjgC",
        "outputId": "ee772166-8a6c-4a68-dcff-438bd735055d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "sp = spm.SentencePieceProcessor(model_proto=model_proto.getvalue())\n",
        "\n",
        "print(sp.encode('This is a test'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[212, 32, 10, 587, 446]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AfxU3ZsoASd"
      },
      "source": [
        "**serialized_model_proto** メソッドを呼ぶことで、モデルのバイト表現を取得できます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5bVqxAHoFYw",
        "outputId": "e7ebe563-4d9c-4258-8011-908ada4837fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model_proto2 = sp.serialized_model_proto()\n",
        "\n",
        "sp2 = spm.SentencePieceProcessor(model_proto=model_proto2)\n",
        "print(sp2.encode('This is a test'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[212, 32, 10, 587, 446]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vCxHx2YoqTv"
      },
      "source": [
        "### Pickle オブジェクト\n",
        "\n",
        "Sentenceiece は、python 標準の pickle 経由でシリアライズ・デシリアライズが可能です。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRYRflJmo2Iw",
        "outputId": "ea18d95b-1e83-4062-af60-9521abd9aafb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import pickle\n",
        "\n",
        "sp = spm.SentencePieceProcessor(model_proto=model_proto.getvalue())\n",
        "print(sp.encode('This is a test'))\n",
        "\n",
        "with open('spm.pickle', 'wb') as f:\n",
        "  pickle.dump(sp, f)\n",
        "\n",
        "with open('spm.pickle', 'rb') as f:\n",
        "  sp2 = pickle.load(f)\n",
        "\n",
        "print(sp2.encode('This is a test'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[212, 32, 10, 587, 446]\n",
            "[212, 32, 10, 587, 446]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZ2GjO5Tmjk9"
      },
      "source": [
        "## BOS, EOS, UNK, PAD トークン"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ts_AwfmJ1Nkc"
      },
      "source": [
        "### BOS, EOS, PAD トークンの表示\n",
        "**bos_id, eos_id, unk_id, pad_id** を用いることで、それぞれの特殊トークンのid を取得できます。**pad_id** はデフォルトでは未定義 (-1) です。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugUwTBhMqjET",
        "outputId": "04761e44-1031-4324-8648-f2278a550635",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "print(sp.unk_id())\n",
        "print(sp.bos_id())\n",
        "print(sp.eos_id())\n",
        "print(sp.pad_id())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "-1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzOC4_Idr2Xp"
      },
      "source": [
        "### BOS, EOS の自動挿入\n",
        "encode 時に, add_bos, add_eos を指定することで自動的に bos/eos トークンを挿入します。reverse を指定すると入力を反転させます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1zUCEdir-qG",
        "outputId": "3438cb73-8fea-45d6-f7a6-55258b8f3ced",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "print(sp.encode('This is a test'))\n",
        "\n",
        "print(sp.encode('This is a test', add_bos=True, add_eos=True))\n",
        "\n",
        "print(sp.encode('This is a test', reverse=True))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[212, 32, 10, 587, 446]\n",
            "[1, 212, 32, 10, 587, 446, 2]\n",
            "[446, 587, 10, 32, 212]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FV3EiD1brc6H"
      },
      "source": [
        "### BOS, EOS, UNK, PAD トークンの書き換え\n",
        "\n",
        "学習時に **pad_id, unk_id, bos_id, eos_id** 引数を使い、それぞれの id を上書きできます。id に -1 が指定されると、その特殊トークンを無効化します。同様に、**pad_piece, bos_piece, eos_iece, unk_piece** を指定することで、内部の文字列表現を書き換えることができます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvKtAgFRtfUU",
        "outputId": "c62ffb22-3d9b-45ee-9c85-59f201445fa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "spm.SentencePieceTrainer.train(input='botchan.txt',\n",
        "                                vocab_size=2000,\n",
        "                                model_prefix='m',\n",
        "                                pad_id=0,\n",
        "                                unk_id=1,\n",
        "                                bos_id=2,\n",
        "                                eos_id=-1,\n",
        "                                pad_piece=\"[PAD]\",\n",
        "                                unk_piece=\"[UNK]\",\n",
        "                                bos_piece=\"[BOS]\")\n",
        "\n",
        "sp = spm.SentencePieceProcessor(model_file='m.model')\n",
        "\n",
        "print(sp.unk_id())\n",
        "print(sp.bos_id())\n",
        "print(sp.eos_id())\n",
        "print(sp.pad_id())\n",
        "\n",
        "print(sp.id_to_piece([0,1,2]))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "-1\n",
            "0\n",
            "['[PAD]', '[UNK]', '[BOS]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jUjWKAYma1a"
      },
      "source": [
        "## UTF8 バイトフォールバック\n",
        "\n",
        "**byte_fallbacl=True** を指定することで、未知文字を utf8 文字に分解する byte_fallback を有効にすることができます。 byte_fallback を有効にするときには、**character_coverage** を 1未満にし、フォールバックされたトークンが学習中に出現するようにすることをおすすめします。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6tG2qIFm9__",
        "outputId": "e18a0043-bcca-44a2-fde5-d87efcb0610c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "spm.SentencePieceTrainer.train(input='botchan.txt',\n",
        "                                vocab_size=2000,\n",
        "                                model_prefix='m', byte_fallback=True)\n",
        "\n",
        "sp = spm.SentencePieceProcessor(model_file='m.model')\n",
        "\n",
        "# 😀 は未知文字。学習データに出現しない\n",
        "pieces = sp.encode('hello world.😀', out_type=str)\n",
        "ids = sp.encode('hello world.😀', out_type=int)\n",
        "\n",
        "# <0xXX> トークンとして出力\n",
        "print(pieces)\n",
        "print(ids)\n",
        "\n",
        "# バイト列から utf8 に戻す\n",
        "print(sp.decode(pieces))\n",
        "print(sp.decode(ids))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['▁he', 'll', 'o', '▁world', '.', '<0xF0>', '<0x9F>', '<0x98>', '<0x80>']\n",
            "[285, 390, 300, 1296, 260, 243, 162, 155, 131]\n",
            "hello world.😀\n",
            "hello world.😀\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imfPyYlVZmxz"
      },
      "source": [
        "## 独自シンボルの定義\n",
        "\n",
        "ユーザ独自のシンボルを定義することができます。例として、\n",
        "[BERT](https://arxiv.org/abs/1810.04805)の [SEP] や [CLS] といった特殊シンボルが挙げられます。 \n",
        "\n",
        "独自シンボルには以下の2種類があります。\n",
        "\n",
        "- **ユーザ定義シンボル**: どのような文脈でも1トークンとして切り出されます。ユーザ辞書のような働きをします。このトークンは入力文字列に出現することができます。\n",
        "- **制御シンボル**:  制御用にid を予約します。制御シンボルは、入力に出現することを想定しておらず、出現しても通常の分割が行われます。ユーザは、予約されたIDをタスクに応じて追加、挿入します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YActMb9sNX6"
      },
      "source": [
        "### ユーザ定義シンボルの例\n",
        "\n",
        "ユーザ定義シンボルは  **user_defined_symbols** パラメータを使いリスト形式で指定します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dngckiPMcWbA",
        "outputId": "7de6ebbb-f32d-45ae-fac0-740a664def6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "spm.SentencePieceTrainer.train(input='botchan.txt',\n",
        "                               model_prefix='m_user',\n",
        "                              user_defined_symbols=['<sep>','<cls>'], vocab_size=2000)\n",
        "\n",
        "sp_user = spm.SentencePieceProcessor(model_file='m_user.model')\n",
        "\n",
        "# <unk>=0, <s>=1, </s>=2, <sep>=3, <cls>=4\n",
        "# ユーザ定義シンボルは、ユーザ辞書のように振る舞い、入力文字列に出現したときには\n",
        "# その単位で切り出されます。\n",
        "print(sp_user.encode('this is a test<sep> hello world<cls>', out_type=str))\n",
        "print(sp_user.encode('this is a test<sep> hello world<cls>', out_type=int))\n",
        "print(sp_user.piece_to_id('<sep>'))  # 3\n",
        "print(sp_user.piece_to_id('<cls>'))  # 4\n",
        "print('3=', sp_user.decode_ids([3]))  # decoded to <sep>\n",
        "print('4=', sp_user.decode_ids([4]))  # decoded to <cls>"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['▁this', '▁is', '▁a', '▁t', 'est', '<sep>', '▁he', 'll', 'o', '▁world', '<cls>']\n",
            "[48, 34, 12, 589, 448, 3, 31, 136, 46, 1042, 4]\n",
            "3\n",
            "4\n",
            "3= <sep>\n",
            "4= <cls>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eT_AA2x7tBUW"
      },
      "source": [
        "### 制御シンボルの例\n",
        "\n",
        "制御シンボルは  **control_symbols** パラメータを使いリスト形式で指定します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5awRJ0y1oYm-",
        "outputId": "5eb6582b-a6dd-4cae-91db-526acf324878",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "spm.SentencePieceTrainer.train(input='botchan.txt',\n",
        "                               model_prefix='m_ctrl',\n",
        "                               control_symbols=['<sep>','<cls>'], vocab_size=2000)\n",
        "\n",
        "sp_ctrl = spm.SentencePieceProcessor(model_file='m_ctrl.model')\n",
        "\n",
        "# 制御シンボルは id のみが予約されます。\n",
        "print(sp_ctrl.encode('this is a test<sep> hello world<cls>', out_type=str))\n",
        "print(sp_ctrl.encode('this is a test<sep> hello world<cls>', out_type=int))\n",
        "print(sp_ctrl.piece_to_id('<sep>'))  # 3\n",
        "print(sp_ctrl.piece_to_id('<cls>'))  # 4\n",
        "print('3=', sp_ctrl.decode_ids([3]))  # decoded to empty\n",
        "print('4=', sp_ctrl.decode_ids([4]))  # decoded to empty"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['▁this', '▁is', '▁a', '▁t', 'est', '<', 'se', 'p', '>', '▁he', 'll', 'o', '▁world', '<', 'c', 'l', 's', '>']\n",
            "[48, 34, 12, 589, 448, 0, 95, 78, 0, 31, 136, 46, 1042, 0, 84, 60, 10, 0]\n",
            "3\n",
            "4\n",
            "3= \n",
            "4= \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ppZck91s0rq"
      },
      "source": [
        "BOS/EOS (&lt;s&gt;, &lt;/s&gt;) は、制御シンボルとして定義されています。これらをユーザ定義シンボルとして再定義することで、入力テキストにこれらのシンボルが含まれると BOS, EOS の動作をするようになります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQoZ8paVhcEL",
        "outputId": "78c4201e-6e7f-4249-c404-ab898febbf23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "spm.SentencePieceTrainer.train(input='botchan.txt',\n",
        "                               model_prefix='m',\n",
        "                               vocab_size=2000)\n",
        "\n",
        "spm.SentencePieceTrainer.train(input='botchan.txt',\n",
        "                               model_prefix='m_bos_as_user',\n",
        "                               user_defined_symbols=['<s>','</s>'],\n",
        "                               vocab_size=2000)\n",
        "\n",
        "# デフォルト動作: <s>, </s> は制御シンボル。入力に含まれていても\n",
        "# 通常通り分割される\n",
        "sp = spm.SentencePieceProcessor(model_file='m.model')\n",
        "print(sp.encode('<s> hello</s>', out_type=str))\n",
        "print(sp.encode('<s> hello</s>', out_type=int))\n",
        "\n",
        "sp = spm.SentencePieceProcessor(model_file='m_bos_as_user.model')\n",
        "# <s>, </s> をユーザシンボルとして再定義。入力に含まれると、BOS/EOSとして扱われる。\n",
        "print(sp.encode('<s> hello</s>', out_type=str))\n",
        "print(sp.encode('<s> hello</s>', out_type=int))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['▁', '<', 's', '>', '▁he', 'll', 'o', '</', 's', '>']\n",
            "[9, 0, 8, 0, 29, 134, 44, 0, 8, 0]\n",
            "['▁', '<s>', '▁he', 'll', 'o', '</s>']\n",
            "[9, 1, 29, 134, 44, 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svXNVp5r1aF2"
      },
      "source": [
        "## サブワード正則化とn-best出力"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vDXA3Q6kjCS"
      },
      "source": [
        "### サブワード正則化\n",
        "**encode** メソッドにて **enable_sampling=True** を指定することで、メソッドの呼び出し毎に確率的に分割の異なる結果を返します。**alpha** パラメータで、分布の偏りを変更できます。小さい値ほど、多様な解を出しやすくなります。\n",
        "\n",
        "**nbest_size** が指定されると、nbest からサンプリングを行います。探索空間が限定されることで、最適解に近い分割に制限しやすくなります。nbest_size は、model_type が unigram のときのみ有効です。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSQp93qflZO3",
        "outputId": "7c858e9e-4be2-4d89-8c99-2a6299e063cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        }
      },
      "source": [
        "print('alpha=0.5')\n",
        "for n in range(10):\n",
        "  print(sp.encode('hello world', out_type=str, enable_sampling=True, alpha=0.5))\n",
        "\n",
        "print('\\nalpha=0.01')\n",
        "for n in range(10):\n",
        "  print(sp.encode('hello world', out_type=str, enable_sampling=True, alpha=0.01))\n",
        "\n",
        "print('\\n alpha=0.01, nbest=3')\n",
        "for n in range(10):\n",
        "  print(sp.encode('hello world', out_type=str, enable_sampling=True, alpha=0.01, nbest_size=3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "alpha=0.5\n",
            "['▁he', 'll', 'o', '▁wor', 'ld']\n",
            "['▁he', 'll', 'o', '▁world']\n",
            "['▁he', 'll', 'o', '▁world']\n",
            "['▁he', 'll', 'o', '▁world']\n",
            "['▁he', 'll', 'o', '▁world']\n",
            "['▁he', 'll', 'o', '▁world']\n",
            "['▁he', 'll', 'o', '▁world']\n",
            "['▁he', 'll', 'o', '▁world']\n",
            "['▁he', 'll', 'o', '▁world']\n",
            "['▁he', 'll', 'o', '▁world']\n",
            "\n",
            "alpha=0.01\n",
            "['▁', 'h', 'e', 'l', 'l', 'o', '▁w', 'or', 'ld']\n",
            "['▁', 'h', 'e', 'l', 'l', 'o', '▁', 'w', 'o', 'r', 'ld']\n",
            "['▁', 'he', 'll', 'o', '▁wor', 'ld']\n",
            "['▁he', 'l', 'l', 'o', '▁w', 'or', 'l', 'd']\n",
            "['▁', 'h', 'e', 'll', 'o', '▁', 'w', 'o', 'r', 'l', 'd']\n",
            "['▁he', 'l', 'l', 'o', '▁', 'wo', 'r', 'ld']\n",
            "['▁', 'h', 'e', 'l', 'l', 'o', '▁w', 'or', 'ld']\n",
            "['▁', 'he', 'l', 'l', 'o', '▁w', 'or', 'ld']\n",
            "['▁', 'h', 'el', 'l', 'o', '▁', 'w', 'or', 'l', 'd']\n",
            "['▁', 'he', 'll', 'o', '▁', 'wo', 'r', 'ld']\n",
            "\n",
            " alpha=0.01, nbest=3\n",
            "['▁', 'he', 'll', 'o', '▁world']\n",
            "['▁he', 'll', 'o', '▁world']\n",
            "['▁he', 'l', 'l', 'o', '▁world']\n",
            "['▁he', 'll', 'o', '▁world']\n",
            "['▁', 'he', 'll', 'o', '▁world']\n",
            "['▁he', 'll', 'o', '▁world']\n",
            "['▁he', 'l', 'l', 'o', '▁world']\n",
            "['▁he', 'll', 'o', '▁world']\n",
            "['▁', 'he', 'll', 'o', '▁world']\n",
            "['▁he', 'll', 'o', '▁world']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ALjYK-szgwL"
      },
      "source": [
        "### nbest 解の出力\n",
        "\n",
        "**nbest_encode_as_ids**, **nbest_encode_as_piece** メソッドで nbest 解を取得することができます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9V1snUZdlb_v",
        "outputId": "06698ac4-ff64-4f55-ef31-5e9289622203",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(sp.nbest_encode_as_ids('hello world', nbest_size=3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[29, 134, 44, 1040], [29, 58, 58, 44, 1040], [9, 787, 134, 44, 1040]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jazO1ig31v7p"
      },
      "source": [
        "# 様々な分割モデル"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cH6cxuVNcDKh"
      },
      "source": [
        "### BPE (Byte pair encoding) モデル\n",
        "\n",
        "**model_type='bpe'** を指定することで、bpe 分割が有効になります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNQxuX4Mc0KY",
        "outputId": "9fec3e8f-26ec-4d40-d6e4-632ce5a420f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "spm.SentencePieceTrainer.train(input='botchan.txt',\n",
        "                                model_prefix='m_bpe',\n",
        "                                vocab_size=2000,\n",
        "                                model_type='bpe')\n",
        "sp_bpe = spm.SentencePieceProcessor(model_file='m_bpe.model')\n",
        "\n",
        "print(sp_bpe.encode('this is a test', out_type=str))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['▁this', '▁is', '▁a', '▁t', 'est']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJXHCoAHoZWg"
      },
      "source": [
        "### 文字・単語分割モデル\n",
        "\n",
        "**model_type='word'**, **model_type='char'** を指定することで、それぞれ単語分割, 文字分割モデルが有効になります。\n",
        "\n",
        "`word` モデルを用いるときは、入力テキストは事前に分かち書きされている必要があります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOAOmQGQpBhg",
        "outputId": "af73fa34-027d-40cb-9201-213b3c76f585",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "spm.SentencePieceTrainer.train(input='botchan.txt',\n",
        "                              model_prefix='m_char', model_type='char',\n",
        "                              vocab_size=400)\n",
        "\n",
        "sp_char = spm.SentencePieceProcessor(model_file='m_char.model')\n",
        "\n",
        "print(sp_char.encode('this is a test.', out_type=str))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['▁', 't', 'h', 'i', 's', '▁', 'i', 's', '▁', 'a', '▁', 't', 'e', 's', 't', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzBiPAm4ljor",
        "outputId": "c963bf57-2eca-4a99-b8eb-c0d48138c087",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "spm.SentencePieceTrainer.train(input='botchan.txt',\n",
        "                              model_prefix='m_word', model_type='word',\n",
        "                              vocab_size=200)\n",
        "\n",
        "sp_word = spm.SentencePieceProcessor(model_file='m_word.model')\n",
        "\n",
        "print(sp_word.encode('this is a test.', out_type=str))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['▁this', '▁is', '▁a', '▁test.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiTgbDypciw9"
      },
      "source": [
        "## テキスト正規化\n",
        "\n",
        "Sentencepiece は、入力文字列を Unicode NFCK で正規化します。ユーザは正規化ルールを変更したり、独自のルールを使って正規化することが可能です。正規化ルールはモデルファイルに埋め込まれます。同一モデルファイルを使う限り、同一の正規化が行われることが保証されれ、前処理の再現性が確保されます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZvkFnw9pt-D"
      },
      "source": [
        "### 定義済みテキスト正規化の変更\n",
        "\n",
        "Sentencepiece は、以下の定義済み的すすと正規化ルールを提供しています。 **normaliation_rule_name=&lt;NAME&gt;** 引数にて、正規化ルールを変更することができます。正規化ルールの情報は、モデルファイルに保存されるため、トークン化時に再指定する必要はありません。\n",
        "\n",
        "- **nmt_nfkc**: Unicode NFKC ルール + 空白まわりの独自ルール (デフォルト)\n",
        "- **nfkc: original**: Unicode NFKC ルール\n",
        "- **nmt_nfkc_cf**: nmt_nfkc + 小文字化\n",
        "- **nfkc_cf**:  nfkc + 小文字化\n",
        "- **identity**: 正規化なし"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSJiwIeBqFcO",
        "outputId": "c567a6c1-064c-4636-d697-6f84da6b72f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "# NFKC+小文字化\n",
        "spm.SentencePieceTrainer.train(input='botchan.txt',\n",
        "                              model_prefix='m_nfkc_cf', vocab_size=2000,\n",
        "                              normalization_rule_name='nfkc_cf')\n",
        "\n",
        "sp = spm.SentencePieceProcessor(model_file='m_nfkc_cf.model')\n",
        "\n",
        "# 小文字化\n",
        "print(sp.encode('ＨＥＬＬＯ　ＷＯＲＬＤ.', out_type=str)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['▁', 'hello', '▁world', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSqHaN3BdQO1"
      },
      "source": [
        "### 独自正規化ルールの作成\n",
        "\n",
        "正規化ルールは、文字列から文字列への書き換え規則として実装されています。書き換えに曖昧性があるばあいは、最長のルールが用いられます。ユーザ定義の正規化ルールはTSVファイルとして与えます。定義済みルールのTSVファイルはdataディレクトリあります([サンプル](https://raw.githubusercontent.com/google/sentencepiece/master/data/nfkc.tsv))。正規化ルールは、FSTにコンパイルされモデルファイルに埋め込まれます。そのため、トークン化の際 TSVファイルを再指定する必要はありません。\n",
        "TSVファイルは、学習時に **normalization_rule_tsv=&lt;FILE&gt;**パラメータとして与えます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHM5aGYTrfXg",
        "outputId": "45d2cfd4-f2fb-49b9-b558-19455aea50e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "def tocode(s):                                                                               \n",
        "    out = [str(hex(ord(c))).replace('0x', 'U+') for c in s]                                                                                                                      \n",
        "    return ' '.join(out)          \n",
        "\n",
        "# TSV format:  書換元 Unicode コードポイント 列 <tab> 書き換え先コードポイント列\n",
        "# 例:  normalize \"don't => do not,  I'm => I am\"\n",
        "with open('normalization_rule.tsv', 'w') as f:\n",
        "  f.write(tocode(\"I'm\") + '\\t' + tocode(\"I am\") + '\\n')\n",
        "  f.write(tocode(\"don't\") + '\\t' + tocode(\"do not\") + '\\n')\n",
        "\n",
        "print(open('normalization_rule.tsv', 'r').read())\n",
        "\n",
        "# 正規化ルールファイルを指定して学習\n",
        "spm.SentencePieceTrainer.train(input='botchan.txt',\n",
        "                               model_prefix='m_custom',\n",
        "                              vocab_size=2000,\n",
        "                                normalization_rule_tsv='normalization_rule.tsv')\n",
        "\n",
        "sp = spm.SentencePieceProcessor(model_file='m_custom.model')\n",
        "\n",
        "print(sp.encode(\"I'm busy\", out_type=str))  # normalzied to `I am busy'\n",
        "print(sp.encode(\"I don't know it.\", out_type=str))  # normalized to 'I do not know it.'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "U+49 U+27 U+6d\tU+49 U+20 U+61 U+6d\n",
            "U+64 U+6f U+6e U+27 U+74\tU+64 U+6f U+20 U+6e U+6f U+74\n",
            "\n",
            "['▁I', '▁am', '▁bu', 's', 'y']\n",
            "['▁I', '▁do', '▁not', '▁know', '▁it', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07FMNoCmglil"
      },
      "source": [
        "## 語彙の制限\n",
        "\n",
        "**set_vocabulary** にトークン集合を渡すことで、encode で使われるトークンを制限することができます。指定された語彙集合のみを使ってトークン化を行います。同制限の詳細については、\n",
        " [subword-nmt page](https://github.com/rsennrich/subword-nmt#best-practice-advice-for-byte-pair-encoding-in-nmt) を御覧ください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2soU1eZhdH_",
        "outputId": "02cee527-dd3b-46ae-bba0-730899951684",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# すべてのトークンを Python list として取得\n",
        "vocabs = [sp.id_to_piece(id) for id in range(sp.get_piece_size())]\n",
        "\n",
        "# 各トークンの出現頻度を計算\n",
        "freq = {}\n",
        "with open('botchan.txt', 'r') as f:\n",
        "    for line in f:\n",
        "        line = line.rstrip()\n",
        "        for piece in sp.encode(line, out_type=str):\n",
        "            freq.setdefault(piece, 0)\n",
        "            freq[piece] += 1\n",
        "          \n",
        "# 1000回以上出現した部分集合を作成\n",
        "vocabs = list(filter(lambda x : x in freq and freq[x] > 1000, vocabs))\n",
        "\n",
        "# 語彙制限前の分割\n",
        "print(sp.encode('this is a test.', out_type=str))\n",
        "\n",
        "# 語彙制限の有効化\n",
        "sp.set_vocabulary(vocabs)\n",
        "print(sp.encode('this is a test.', out_type=str))\n",
        "\n",
        "# 語彙制限をリセット\n",
        "sp.reset_vocabulary()\n",
        "print(sp.encode('this is a test.', out_type=str))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['▁this', '▁is', '▁a', '▁t', 'est', '.']\n",
            "['▁', 't', 'h', 'i', 's', '▁', 'i', 's', '▁a', '▁', 't', 'e', 's', 't', '.']\n",
            "['▁this', '▁is', '▁a', '▁t', 'est', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8rQLqCTHk40"
      },
      "source": [
        "## 複数単語にまたがるトークンの学習\n",
        "\n",
        "デフォルト動作では、空白をトークンの区切り制約とするため、複数の単語(ここでの単語とは、空白で区切られたトークンを意味する)にまたがるトークンを抽出することはありません。\n",
        "\n",
        "**split_by_whtespace=False** を指定して学習することで、この動作を停止し、複数の単語にまたがるトークンを抽出するようになります。ただし、中国語や日本語の場合は、文が空白で区切られていないため、このパラメータによって動作が大きく変わることはないでしょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lf5Fs_pPIKif",
        "outputId": "f9b399b3-59fd-4690-aa96-4e64a08a392b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "source": [
        "import re\n",
        "\n",
        "spm.SentencePieceTrainer.train(input='botchan.txt',\n",
        "                                model_prefix='m_cross_word',\n",
        "                                vocab_size=2000,\n",
        "                                split_by_whitespace=False)\n",
        "\n",
        "sp = spm.SentencePieceProcessor(model_file='m_cross_word.model')\n",
        "\n",
        "# 学習されたトークンをPython list として取得\n",
        "vocabs = [sp.id_to_piece(id) for id in range(sp.piece_size())]\n",
        "\n",
        "# 複数単語にまたがるトークンを出力\n",
        "for piece in vocabs[0:500]:\n",
        "    if re.match('\\w+▁\\w+', piece):\n",
        "        print(piece)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ed▁to\n",
            "roject▁Gutenberg\n",
            "s▁of\n",
            "ing▁the\n",
            "ed▁me\n",
            "ed▁the\n",
            "ed▁in\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWjA7yOX1Rlg"
      },
      "source": [
        "## 頻度付き単語集合からの学習\n",
        "\n",
        "Sentencepiece は、頻度付きの単語集合からのサブワード学習をサポートしています。\n",
        "\n",
        "学習データとして、1カラム目が単語、2カラム目が頻度となるようなTSVファイルを作成します。\n",
        "学習時に、**input_format='tsv'** を指定することで、入力ファイルのフォーマットをTSVに変更します。 TSVファイルが入力の場合は、**split_by_whtespace=true** が設定されているもととみなします。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7F349Sd2Bzg",
        "outputId": "d3c592af-78a6-420c-fb0d-49e6bff7451f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "freq={}\n",
        "with open('botchan.txt', 'r') as f:\n",
        "  for line in f:\n",
        "    line = line.rstrip()\n",
        "    for piece in line.split():\n",
        "      freq.setdefault(piece, 0)\n",
        "      freq[piece] += 1\n",
        "            \n",
        "with open('word_freq_list.tsv', 'w') as f:\n",
        "  for k, v in freq.items():\n",
        "    f.write('%s\\t%d\\n' % (k, v))\n",
        "  \n",
        "spm.SentencePieceTrainer.train(input='word_freq_list.tsv',\n",
        "                               input_format='tsv',\n",
        "                               model_prefix='m_word_freq',\n",
        "                               vocab_size=2000)\n",
        "sp = spm.SentencePieceProcessor(model_file='m_word_freq.model')\n",
        "\n",
        "print(sp.encode('this is a test.', out_type=str))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['▁this', '▁is', '▁a', '▁t', 'est', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiWMoTpA-pHx"
      },
      "source": [
        "## トークンの文字オフセットの取得\n",
        "\n",
        "Sentencepiece は、各トークンが元の入力文のどの位置に対応するか、utf8 byte 単位でオフセットを保持しています。オフセット情報は、文字列のハイライト等に利用可能です。\n",
        "\n",
        "オフセット情報を利用するには、protobuf モジュールと sentencepiece_pb2.py が必要となります。SentencePieceProcessor の **encode_as_serialized_proto** 経由で、シリアライズされたSentencePieceText proto が取得できます。同バイト列をデシリアライズし、SentencePieceText proto に変換します。SentencePieceText は、オフセット以外にも、トークンの文字列表現、ID等が保持されています。\n",
        "\n",
        "SentencePieceText のproto 定義は [こちら](https://github.com/google/sentencepiece/blob/3be3f2e11e2bb923c579c6be5e7335809341587f/src/sentencepiece.proto#L23)にあります。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTYrvL6KkmVK",
        "outputId": "81f1676e-d5b3-412e-ce59-67d51f80471d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        }
      },
      "source": [
        "!pip install protobuf\n",
        "!wget https://raw.githubusercontent.com/google/sentencepiece/master/python/sentencepiece_pb2.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (3.12.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf) (50.3.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf) (1.15.0)\n",
            "--2020-10-19 15:00:55--  https://raw.githubusercontent.com/google/sentencepiece/master/python/sentencepiece_pb2.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7382 (7.2K) [text/plain]\n",
            "Saving to: ‘sentencepiece_pb2.py’\n",
            "\n",
            "sentencepiece_pb2.p 100%[===================>]   7.21K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-10-19 15:00:55 (83.8 MB/s) - ‘sentencepiece_pb2.py’ saved [7382/7382]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdRy9sEvk7zw",
        "outputId": "95b7ff3e-5b3d-458c-85af-0a68fcbc2b71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import sentencepiece_pb2\n",
        "\n",
        "spm.SentencePieceTrainer.train(input='botchan.txt',\n",
        "                                model_prefix='m',\n",
        "                                vocab_size=2000)\n",
        "\n",
        "sp = spm.SentencePieceProcessor(model_file='m.model')\n",
        "\n",
        "# One best result\n",
        "spt = sentencepiece_pb2.SentencePieceText()\n",
        "spt.ParseFromString(sp.encode_as_serialized_proto('ｈｅｌｌｏ')) # Full width hello\n",
        "\n",
        "# begin/end (offsets) are pointing to the original input.\n",
        "print('============== one best ================')\n",
        "print(spt)\n",
        "\n",
        "# Nbest results\n",
        "nspt = sentencepiece_pb2.NBestSentencePieceText()\n",
        "nspt.ParseFromString(sp.nbest_encode_as_serialized_proto('ｈｅｌｌｏ', 3))\n",
        "print('============== nbest ================')\n",
        "print(nspt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "============== one best ================\n",
            "text: \"\\357\\275\\210\\357\\275\\205\\357\\275\\214\\357\\275\\214\\357\\275\\217\"\n",
            "pieces {\n",
            "  piece: \"\\342\\226\\201he\"\n",
            "  id: 29\n",
            "  surface: \"\\357\\275\\210\\357\\275\\205\"\n",
            "  begin: 0\n",
            "  end: 6\n",
            "}\n",
            "pieces {\n",
            "  piece: \"ll\"\n",
            "  id: 134\n",
            "  surface: \"\\357\\275\\214\\357\\275\\214\"\n",
            "  begin: 6\n",
            "  end: 12\n",
            "}\n",
            "pieces {\n",
            "  piece: \"o\"\n",
            "  id: 44\n",
            "  surface: \"\\357\\275\\217\"\n",
            "  begin: 12\n",
            "  end: 15\n",
            "}\n",
            "\n",
            "============== nbest ================\n",
            "nbests {\n",
            "  text: \"\\357\\275\\210\\357\\275\\205\\357\\275\\214\\357\\275\\214\\357\\275\\217\"\n",
            "  pieces {\n",
            "    piece: \"\\342\\226\\201he\"\n",
            "    id: 29\n",
            "    surface: \"\\357\\275\\210\\357\\275\\205\"\n",
            "    begin: 0\n",
            "    end: 6\n",
            "  }\n",
            "  pieces {\n",
            "    piece: \"ll\"\n",
            "    id: 134\n",
            "    surface: \"\\357\\275\\214\\357\\275\\214\"\n",
            "    begin: 6\n",
            "    end: 12\n",
            "  }\n",
            "  pieces {\n",
            "    piece: \"o\"\n",
            "    id: 44\n",
            "    surface: \"\\357\\275\\217\"\n",
            "    begin: 12\n",
            "    end: 15\n",
            "  }\n",
            "  score: -17.582029342651367\n",
            "}\n",
            "nbests {\n",
            "  text: \"\\357\\275\\210\\357\\275\\205\\357\\275\\214\\357\\275\\214\\357\\275\\217\"\n",
            "  pieces {\n",
            "    piece: \"\\342\\226\\201he\"\n",
            "    id: 29\n",
            "    surface: \"\\357\\275\\210\\357\\275\\205\"\n",
            "    begin: 0\n",
            "    end: 6\n",
            "  }\n",
            "  pieces {\n",
            "    piece: \"l\"\n",
            "    id: 58\n",
            "    surface: \"\\357\\275\\214\"\n",
            "    begin: 6\n",
            "    end: 9\n",
            "  }\n",
            "  pieces {\n",
            "    piece: \"l\"\n",
            "    id: 58\n",
            "    surface: \"\\357\\275\\214\"\n",
            "    begin: 9\n",
            "    end: 12\n",
            "  }\n",
            "  pieces {\n",
            "    piece: \"o\"\n",
            "    id: 44\n",
            "    surface: \"\\357\\275\\217\"\n",
            "    begin: 12\n",
            "    end: 15\n",
            "  }\n",
            "  score: -22.676969528198242\n",
            "}\n",
            "nbests {\n",
            "  text: \"\\357\\275\\210\\357\\275\\205\\357\\275\\214\\357\\275\\214\\357\\275\\217\"\n",
            "  pieces {\n",
            "    piece: \"\\342\\226\\201\"\n",
            "    id: 9\n",
            "    surface: \"\"\n",
            "    begin: 0\n",
            "    end: 0\n",
            "  }\n",
            "  pieces {\n",
            "    piece: \"he\"\n",
            "    id: 787\n",
            "    surface: \"\\357\\275\\210\\357\\275\\205\"\n",
            "    begin: 0\n",
            "    end: 6\n",
            "  }\n",
            "  pieces {\n",
            "    piece: \"ll\"\n",
            "    id: 134\n",
            "    surface: \"\\357\\275\\214\\357\\275\\214\"\n",
            "    begin: 6\n",
            "    end: 12\n",
            "  }\n",
            "  pieces {\n",
            "    piece: \"o\"\n",
            "    id: 44\n",
            "    surface: \"\\357\\275\\217\"\n",
            "    begin: 12\n",
            "    end: 15\n",
            "  }\n",
            "  score: -25.08266258239746\n",
            "}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}