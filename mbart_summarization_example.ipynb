{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mbart_summarization_example.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/otanet/NLP_seminar_20201022/blob/main/mbart_summarization_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aI216lviXweY"
      },
      "source": [
        "# mBARTを用いた要約生成の例\n",
        "*   本ノートブックでは，日本語要約データにfinetuneされたmBARTモデルで要約を生成する例を示します．\n",
        "*   huggingfaceの[transformers](https://github.com/huggingface/transformers)を使用します．\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ivx1RJ1H00v"
      },
      "source": [
        "# 独自データを使ってfine-tuningする場合"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rV5HS1fkH9Ev"
      },
      "source": [
        "transformersにサンプルコードがあるのでそちらを参考にしてください\n",
        "\n",
        "1.   git clone https://github.com/huggingface/transformers.git\n",
        "2.   %cd /path/transformers/examples/seq2seq\n",
        "3.   [examplesの説明](https://github.com/huggingface/transformers/tree/master/examples)に従って必要なライブラリをインストール (important notes)\n",
        "3.   train_mbart_cc25_enro.shを参考に日本語用に設定を一部変更します\n",
        "4.   --src_lang en_XX --tgt_lang ro_ROの部分で言語を指定します．日本語の場合は--src_lang ja_XX --tgt_lang ja_XXとしてください\n",
        "5.   --data_dir でデータの場所をしてください．データはtrain.source, train.target, val.source, val.target,test.source, test.targetの6種類が必要です\n",
        "6.   --max_source_length で扱う入力長の最大値を設定できます．マシンによってはこの値を小さく設定してください．その他マシンによって適宜設定を変更してください．\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMBJK4o0HwAH"
      },
      "source": [
        "# 学習済モデルを使って生成を試す場合"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1tiDr0jUpdj"
      },
      "source": [
        "## 環境のセットアップ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8EBbXtT8oV6",
        "outputId": "624c8d07-3ca0-41c5-a4e4-185d1be5665d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        }
      },
      "source": [
        "!pip install transformers==3.3.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers==3.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/22/aff234f4a841f8999e68a7a94bdd4b60b4cebcfeca5d67d61cd08c9179de/transformers-3.3.1-py3-none-any.whl (1.1MB)\n",
            "\r\u001b[K     |▎                               | 10kB 20.0MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 6.5MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 7.5MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40kB 7.7MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51kB 7.0MB/s eta 0:00:01\r\u001b[K     |█▉                              | 61kB 7.7MB/s eta 0:00:01\r\u001b[K     |██▏                             | 71kB 8.7MB/s eta 0:00:01\r\u001b[K     |██▌                             | 81kB 8.9MB/s eta 0:00:01\r\u001b[K     |██▉                             | 92kB 8.8MB/s eta 0:00:01\r\u001b[K     |███                             | 102kB 9.3MB/s eta 0:00:01\r\u001b[K     |███▍                            | 112kB 9.3MB/s eta 0:00:01\r\u001b[K     |███▊                            | 122kB 9.3MB/s eta 0:00:01\r\u001b[K     |████                            | 133kB 9.3MB/s eta 0:00:01\r\u001b[K     |████▎                           | 143kB 9.3MB/s eta 0:00:01\r\u001b[K     |████▋                           | 153kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████                           | 163kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 174kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 184kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 194kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 204kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 215kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 225kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████                         | 235kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 245kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 256kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████                        | 266kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 276kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 286kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████                       | 296kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 307kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 317kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 327kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 337kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 348kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 358kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 368kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 378kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 389kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████                    | 399kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 409kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 419kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 430kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 440kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 450kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 460kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 471kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 481kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 491kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 501kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 512kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 522kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████                | 532kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 542kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 552kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 563kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 573kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 583kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 593kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 604kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 614kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 624kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 634kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 645kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 655kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 665kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 675kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 686kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 696kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 706kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 716kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 727kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 737kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 747kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 757kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 768kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 778kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 788kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 798kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 808kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 819kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 829kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 839kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 849kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 860kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 870kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 880kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 890kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 901kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 911kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 921kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 931kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 942kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 952kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 962kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 972kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 983kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 993kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.0MB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.0MB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.0MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.0MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.0MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.1MB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1MB 9.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.1) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.1) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.1) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.1) (1.18.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.1) (4.41.1)\n",
            "Collecting tokenizers==0.8.1.rc2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 33.0MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 54.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.1) (0.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.1) (20.4)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 47.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.3.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.3.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.3.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.3.1) (2020.6.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.3.1) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.3.1) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.3.1) (0.16.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.3.1) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=636b1b55758ce0c2743e282305ec600176c1a1c6bbef61cee97b3b662aad1f45\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc2 transformers-3.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czat2iaJ81Wx"
      },
      "source": [
        "from transformers import MBartForConditionalGeneration, MBartTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnlYMWeCTUwd"
      },
      "source": [
        "## 学習済モデル・トークナイザ読み込み"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74UO9oog7hfe"
      },
      "source": [
        "発表中のデモで使用したlivedoor3行要約データ約10000件で学習済のモデルを下記においたので，学習済モデルで試す場合は[こちらのモデル](https://drive.google.com/file/d/1YMCVZd8sNvozegFbR73elDaeyNV7gY9U/view?usp=sharing)をダウンロード＆展開して指定してください\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "1.   上記のlivedoor_sample.tar.gzをダウンロード \n",
        "2.   tar zxvf livedoor_sample.tar.gz\n",
        "3.   best_tfmrというディレクトリができる\n",
        "4.   Colabの作業環境のどこかに配置．配置した場所を/path/model/best_tfmrとする\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2h4yxBWL9EHt",
        "outputId": "5119b01c-eb65-43f5-912e-f2ffc4a48270",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = MBartForConditionalGeneration.from_pretrained(\"/path/model/best_tfmr\")\n",
        "tokenizer = MBartTokenizer.from_pretrained(\"/path/model/best_tfmr\")\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MBartForConditionalGeneration(\n",
              "  (model): BartModel(\n",
              "    (shared): Embedding(250027, 1024, padding_idx=1)\n",
              "    (encoder): BartEncoder(\n",
              "      (embed_tokens): Embedding(250027, 1024, padding_idx=1)\n",
              "      (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
              "      (layers): ModuleList(\n",
              "        (0): EncoderLayer(\n",
              "          (self_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): EncoderLayer(\n",
              "          (self_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): EncoderLayer(\n",
              "          (self_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): EncoderLayer(\n",
              "          (self_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): EncoderLayer(\n",
              "          (self_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): EncoderLayer(\n",
              "          (self_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (6): EncoderLayer(\n",
              "          (self_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (7): EncoderLayer(\n",
              "          (self_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (8): EncoderLayer(\n",
              "          (self_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (9): EncoderLayer(\n",
              "          (self_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (10): EncoderLayer(\n",
              "          (self_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (11): EncoderLayer(\n",
              "          (self_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): BartDecoder(\n",
              "      (embed_tokens): Embedding(250027, 1024, padding_idx=1)\n",
              "      (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
              "      (layers): ModuleList(\n",
              "        (0): DecoderLayer(\n",
              "          (self_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): DecoderLayer(\n",
              "          (self_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): DecoderLayer(\n",
              "          (self_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): DecoderLayer(\n",
              "          (self_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): DecoderLayer(\n",
              "          (self_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): DecoderLayer(\n",
              "          (self_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (6): DecoderLayer(\n",
              "          (self_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (7): DecoderLayer(\n",
              "          (self_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (8): DecoderLayer(\n",
              "          (self_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (9): DecoderLayer(\n",
              "          (self_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (10): DecoderLayer(\n",
              "          (self_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (11): DecoderLayer(\n",
              "          (self_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9Ix6n8xTFQw"
      },
      "source": [
        "## ソーステキストの指定"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIB-QeXMTMlT"
      },
      "source": [
        "*   ソーステキストを指定して，トークナイズ＆id化する\n",
        "*   下記のARTICLE_TO_SUMMARIZE =\"\"の中身を自由に変更して要約を実行\n",
        "*   今回は例文として[こちら](https://www3.nhk.or.jp/news/html/20201020/k10012671361000.html?utm_int=news_contents_news-main_007)のニューステキスト（一部）を用いる\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "上記ニュースサイトから一部抜粋したソーステキスト\n",
        "\n",
        "世界を代表するフランスの建築家、ル・コルビュジエが90年余り前に改修した船が、日本の建築家らの支援でパリのセーヌ川から引き揚げられ、今後船上ギャラリーとして再生されることになりました。この船は、世界文化遺産に登録された東京の国立西洋美術館などの設計で知られる、ル・コルビュジエが90年余り前に改修したもので、水平な連続窓などみずから提唱した近代建築の要素が盛り込まれフランスの文化財にも指定されています\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3DQ7XQ9SuE6",
        "outputId": "42f65366-0019-4e07-caf7-2d9b01011429",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "ARTICLE_TO_SUMMARIZE=\"世界を代表するフランスの建築家、ル・コルビュジエが90年余り前に改修した船が、日本の建築家らの支援でパリのセーヌ川から引き揚げられ、今後船上ギャラリーとして再生されることになりました。この船は、世界文化遺産に登録された東京の国立西洋美術館などの設計で知られる、ル・コルビュジエが90年余り前に改修したもので、水平な連続窓などみずから提唱した近代建築の要素が盛り込まれフランスの文化財にも指定されています\"\n",
        "batch = tokenizer.prepare_seq2seq_batch(src_texts=[ARTICLE_TO_SUMMARIZE], src_lang=\"ja_XX\", max_target_length=1024, truncation=True).to('cuda')\n",
        "print(batch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([[     6,   3221,    251,   6959,   1786, 155480,    154,  31412,   1433,\n",
            "             37,   5283,   1925,   9368,   5283,  20891,  55545,  13523,  11264,\n",
            "            281,   5039,    470,  11152,   2694,  20362,  10623,   7981,   2419,\n",
            "          15065,    281,     37,  44554,  31412,   1433,   4182,    154,  23795,\n",
            "            507,  21348,   5677,    154,  26221,   4725, 182537,  13399,   1309,\n",
            "          59917,  50464,  39187,  31296,     37,  71312,  15065,    575, 224801,\n",
            "           8697,  21711,   5750,  85665,   9576, 212914,     30,   3619,  15065,\n",
            "            342,     37,   3221,   4278, 224453, 160450,   8346,  22888,    154,\n",
            "         186026, 182364, 170853,  18009,   6156,    507,   5085,  14446,     37,\n",
            "           5283,   1925,   9368,   5283,  20891,  55545,  13523,  11264,    281,\n",
            "           5039,    470,  11152,   2694,  20362,  10623,   7981, 128647,    507,\n",
            "             37,  13368,   1308, 181797, 182944,   5546,   6095,  21862,   1309,\n",
            "           9744,  34993,   2419, 184058,  31412,    154,  67147,    281, 203291,\n",
            "         167080, 155480,    154,   4278,  32450,   6013,  28755,  29667,      2,\n",
            "         250012]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1324: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRZCL48_S7-U"
      },
      "source": [
        "## 要約の生成"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WeGCEPddr7O"
      },
      "source": [
        "*   num_beamsを変えることで，どの範囲まで探索するかを変えることができる（この値を大きくするほど推論は遅くなる）\n",
        "*   max_lengthで要約の出力最大長を指定する．この長さを超える場合は強制的に終了する\n",
        "\n",
        "\n",
        "---\n",
        "生成される要約テキスト（上記サンプルの場合）\n",
        "\n",
        "ル・コルビュジエが90年余り前に改修した船が、日本の建築家らの支援で引き揚げられた。水平な連続窓など、みずから提唱した近代建築の要素が盛り込まれている。今後船上ギャラリーとして再生されるとのこと。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fM_0DEYS4Yu",
        "outputId": "310319cf-94e4-4980-cdf0-777d3dd9a3f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "translated_tokens = model.generate(**batch, decoder_start_token_id=tokenizer.lang_code_to_id[\"ja_XX\"], max_length=100, early_stopping=True, num_beams=4)\n",
        "translation = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
        "print(translation)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ル・コルビュジエが90年余り前に改修した船が、日本の建築家らの支援で引き揚げられた。水平な連続窓など、みずから提唱した近代建築の要素が盛り込まれている。今後船上ギャラリーとして再生されるとのこと。\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}